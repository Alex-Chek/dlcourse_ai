{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-4ab6e7a8ed1f>:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
      "<ipython-input-3-4ab6e7a8ed1f>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n"
     ]
    }
   ],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_11=1e-5\n",
    "aaa = np.array([3.0])\n",
    "aa = np.array([3.0, 2.0])\n",
    "# bb = np.array([2.0, 1.0])\n",
    "cc = np.array([[3.0, 2.0], [1.0, 0.0]])\n",
    "# float(aaa * aaa)\n",
    "# ((aaa[(0,)] + delta_11)*(aaa[(0,)] + delta_11) - (aaa[(0,)] - delta_11)*(aaa[(0,)] - delta_11)) / (2*delta_11)\n",
    "# (aaa - delta_11)*(aaa - delta_11)\n",
    "# float(2*aaa)\n",
    "# (np.sum(aa) - np.sum(bb)) #/ 2.0\n",
    "\n",
    "# (np.sum(cc + delta_11) - np.sum(cc))/ (delta_11)\n",
    "# ((aa[0] + delta_11) - (aa[0] - delta_11)) / (2*delta_11)\n",
    "# np.sum(((cc + delta_11) - (cc - delta_11)) / (2*delta_11))\n",
    "# (cc[:, 1])\n",
    "# aa + np.array([delta_11])\n",
    "# (np.sum(cc + np.array(delta_11)) - np.sum(cc - delta_11) )/ (2*np.array(delta_11))\n",
    "# for i in aaa:\n",
    "#     print(i[(0,)])\n",
    "# np.array(cc[(0,0)])\n",
    "# np.sum((cc + np.array(delta_11))[(0,0)]) - (cc - np.array(delta_11))[(0,0)]\n",
    "# for i in [cc]:\n",
    "#     i[(0,0)] = (i[(0,0)] + delta_11 - (i[(0,0)] - delta_11)) / (2*np.array(delta_11))\n",
    "# np.array([float((i[(0,0)] + delta_11 - (i[(0,0)] - delta_11)) / (2*np.array(delta_11)))] + [0] * 3).reshape(2,2)\n",
    "# np.array(cc.shape) - np.array((1, 0))\n",
    "z = np.zeros_like(cc)\n",
    "\n",
    "z[(0,1)] = ((cc[(0,0)] + delta_11 - (cc[(0,0)] - delta_11)) / (2*np.array(delta_11)))\n",
    "(sum(aa+delta_11) - sum(aa-delta_11) ) /(2 * delta_11)\n",
    "# np.array(cc[(0, 0)]).reshape(cc.shape)\n",
    "c = np.full((cc.shape), None)\n",
    "# c[(0,1)] = float((cc[(0,0)] + delta_11 - (cc[(0,0)] - delta_11)) / (2*np.array(delta_11)))\n",
    "# cc, np.sum(cc, ())\n",
    "# (c) -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def qub(x):\n",
    "    return float(x**3), 3 * x**2\n",
    "\n",
    "check_gradient(qub, np.array([344.0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([1, 2])\n",
    "# print(np.exp(1), np.exp(2))\n",
    "\n",
    "# print(np.exp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array([2, 4, 6, 77])\n",
    "# a[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(6.69254912e-03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qq = np.array([1, 0, 0])\n",
    "\n",
    "# exponents = np.exp(qq)\n",
    "# probs = exponents / np.sum(exponents)\n",
    "# h = -1 * np.log(probs)\n",
    "# # loss = h[target_index]\n",
    "# np.gradient(h) + np.gradient(probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2. -1.  1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-4f517e9443e2>:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "<ipython-input-6-4f517e9443e2>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "# num_classes = 4\n",
    "# batch_size = 3\n",
    "# predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "print(predictions)\n",
    "# target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "zeros = np.zeros_like(predictions)\n",
    "# zeros[:, target_index] = 1\n",
    "print(zeros)\n",
    "zeros[np.arange(1) ,target_index.T] = 1\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2. -1.  1.]\n",
      " [ 1.  2. -1. -1.]\n",
      " [ 1.  0.  1.  1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-3a3cbf7a341d>:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "<ipython-input-9-3a3cbf7a341d>:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 3]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "# num_classes = 4\n",
    "# batch_size = 1\n",
    "# predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "print(predictions)\n",
    "# target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "target_index.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "zeros = np.zeros_like(predictions)\n",
    "# zeros[:, target_index] = 1\n",
    "print(zeros)\n",
    "zeros[np.arange(3) ,target_index.T] = 1\n",
    "print(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "============\n",
      "Gradient check passed!\n",
      "============\n",
      "OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-3359e4053b94>:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "<ipython-input-10-3359e4053b94>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
      "<ipython-input-10-3359e4053b94>:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
      "<ipython-input-10-3359e4053b94>:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "print(\"===\"*4)\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "print(\"===\"*4)\n",
    "# # Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(np.array([1,0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bb = np.array([[20,0,0], [1000, 0, 0]])\n",
    "bb = np.array([[20,0,0], [1000, 0, 0]])#, dtype=np.float64)\n",
    "m = np.array(np.max(bb, axis=1)).reshape(bb.shape[0], 1)\n",
    "print(m)\n",
    "# bb -= np.max(bb, axis=1)\n",
    "bb -= m\n",
    "\n",
    "print(bb)\n",
    "\n",
    "# np.exp (z) / np.sum (np.exp (z), axis = 1, keepdims = True)\n",
    "\n",
    "# bb = np.exp(bb)\n",
    "print('bb', bb)\n",
    "s1 = np.exp(bb) / np.sum(np.exp(bb), axis=1).reshape(bb.shape[0], 1)#, keepdims=True).reshape(bb.shape[0], 1)\n",
    "# s1 = bb / np.sum(bb, axis=1).reshape(bb.shape[0], 1)#, keepdims=True).reshape(bb.shape[0], 1)\n",
    "\n",
    "s2 = np.exp(bb) / np.sum(np.exp(bb), axis=1, keepdims=True).reshape(bb.shape[0], 1)\n",
    "\n",
    "# s = bb / np.sum(bb, axis=1).reshape(bb.shape[0], 1)\n",
    "# # np.exp(bb)\n",
    "# print('**',s1)\n",
    "# np.where(myarray>0, np.log(myarray), 0)\n",
    "# s1 = (bb) - np.log(bb)\n",
    "s1 = -1 * np.log(s1)\n",
    "\n",
    "s2 = -1 * np.log(s2)\n",
    "# s = -1 * np.where(s > 0, np.log(s), 0)\n",
    "\n",
    "# loc = np.where(s>0)\n",
    "\n",
    "print('**', s1)\n",
    "print('***', s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW: \n",
      " [[-0.44039854  0.44039854]\n",
      " [-0.4166856   0.4166856 ]\n",
      " [ 0.46411148 -0.46411148]]\n",
      "Gradient check passed!\n",
      "[1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-222-054f9cd00594>:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
      "<ipython-input-222-054f9cd00594>:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
      "<ipython-input-222-054f9cd00594>:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.ones(batch_size, dtype=np.int)\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "print('dW: \\n', dW)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)\n",
    "print(target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0877576813083574,\n",
       " array([[-0.44039854,  0.44039854],\n",
       "        [-0.4166856 ,  0.4166856 ],\n",
       "        [ 0.46411148, -0.46411148]]))"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1., -1.,  1.],\n",
       "        [ 0.,  1.,  1.]]),\n",
       " array([[ 1.,  2.],\n",
       "        [-1.,  1.],\n",
       "        [ 1.,  2.]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  1.,  1.],\n",
       "       [ 0.,  1.,  2.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X*W.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 3),\n",
       " array([[-1., -1.,  1.],\n",
       "        [ 0.,  1.,  1.]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 2),\n",
       " array([[ 1.,  2.],\n",
       "        [-1.,  1.],\n",
       "        [ 1.,  2.]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.],\n",
       "       [ 0.,  3.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx = np.dot(X,W)\n",
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  5.],\n",
       "       [-1.,  4.],\n",
       "       [ 1.,  5.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.dot(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 2),\n",
       " array([[ 1.,  1.],\n",
       "        [-1.,  0.],\n",
       "        [ 1.,  2.]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW.shape, dW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  2.],\n",
       "        [-1.,  1.],\n",
       "        [ 1.,  2.]]),\n",
       " array([[1., 4.],\n",
       "        [1., 1.],\n",
       "        [1., 4.]]))"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, W**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target index:  [0] [3] [2] [0] [1] [3] [9] [7] [5] [8] [3] [3] [2] [6] [0] [4] [6] [9] [3] [8] [4] [7] [6] [1] [5] [2] [1] [5] [3] [2] [4] [1] [2] [3] [4] [5] [4] [5] [7] [6] [2] [4] [5] [8] [4] [6] [1] [4] [9] [9] [5] [2] [4] [4] [4] [3] [0] [7] [5] [2] [5] [8] [5] [2] [0] [1] [7] [2] [2] [8] [2] [5] [2] [8] [8] [2] [1] [0] [4] [1] [5] [1] [1] [2] [7] [9] [9] [9] [0] [1] [9] [6] [7] [3] [3] [2] [1] [0] [1] [1] [8] [7] [2] [7] [4] [0] [1] [5] [4] [6] [0] [3] [8] [3] [2] [3] [1] [0] [2] [7] [1] [1] [1] [4] [4] [8] [0] [8] [0] [5] [2] [4] [4] [5] [6] [0] [1] [1] [7] [0] [7] [2] [8] [2] [5] [7] [7] [9] [5] [5] [1] [6] [2] [4] [6] [3] [4] [5] [2] [9] [1] [3] [2] [0] [1] [7] [6] [3] [8] [4] [0] [3] [3] [0] [4] [5] [6] [7] [9] [7] [1] [4] [7] [2] [0] [1] [7] [5] [8] [7] [1] [8] [1] [7] [2] [6] [6] [5] [9] [7] [0] [5] [3] [1] [6] [9] [2] [8] [8] [4] [9] [1] [0] [2] [1] [6] [8] [8] [4] [2] [3] [7] [0] [2] [5] [3] [8] [3] [3] [8] [1] [3] [0] [8] [2] [7] [8] [1] [2] [9] [2] [7] [5] [7] [1] [5] [9] [4] [4] [8] [2] [5] [5] [7] [8] [7] [3] [2] [2] [1] [5] [8] [1] [7] [9] [1] [5] [4] [0] [8] [6] [4] [6] [1] [7] [7] [3] [2] [5] [7] [2] [9] [9] [5] [1] [6] [8] [8] [1] [5] [1] [0] [8] [0] [0] [6] [0] [8] [7] [8] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3042735647657424  >loss\n",
      "2.3026139372753365  >loss\n",
      "2.3008809008497315  >loss\n",
      "2.3021941228908696  >loss\n",
      "2.3019968312071524  >loss\n",
      "2.3021963714818794  >loss\n",
      "2.303030936986767  >loss\n",
      "2.3042356381529556  >loss\n",
      "2.301876008904793  >loss\n",
      "2.3017302691866135  >loss\n",
      "2.3019334309322765  >loss\n",
      "2.300914239856158  >loss\n",
      "2.302144691489796  >loss\n",
      "2.3021532993455813  >loss\n",
      "2.30274662907629  >loss\n",
      "2.301056915167614  >loss\n",
      "2.3028853001270053  >loss\n",
      "2.302407823540684  >loss\n",
      "2.303839001680802  >loss\n",
      "2.300676741532771  >loss\n",
      "2.302078215341486  >loss\n",
      "2.302769556883016  >loss\n",
      "2.300765580177939  >loss\n",
      "2.302029255813838  >loss\n",
      "2.300502490594259  >loss\n",
      "2.2996917167210147  >loss\n",
      "2.303075141768147  >loss\n",
      "2.303118069151533  >loss\n",
      "2.302220905934523  >loss\n",
      "2.3013532805600776  >loss\n",
      "2.3007166408469004  >loss\n",
      "2.304359344629771  >loss\n",
      "2.3003194874759663  >loss\n",
      "2.3027958779626014  >loss\n",
      "2.3008598248409524  >loss\n",
      "2.3006692860955544  >loss\n",
      "2.301714072542749  >loss\n",
      "2.299766709394111  >loss\n",
      "2.3015610891512694  >loss\n",
      "2.3038901042157685  >loss\n",
      "2.2994316377158075  >loss\n",
      "2.304782340403993  >loss\n",
      "2.3022106263609525  >loss\n",
      "2.2993821113562314  >loss\n",
      "2.2998132358939656  >loss\n",
      "2.3031516652135897  >loss\n",
      "2.301455839431798  >loss\n",
      "2.3017637828182447  >loss\n",
      "2.3012463023151075  >loss\n",
      "2.3037186732020714  >loss\n",
      "2.3005735300736636  >loss\n",
      "2.300981911792247  >loss\n",
      "2.3029380915586284  >loss\n",
      "2.300454229528811  >loss\n",
      "2.3025355835307364  >loss\n",
      "2.2985453918840695  >loss\n",
      "2.3038969777773852  >loss\n",
      "2.30024341134325  >loss\n",
      "2.300811884931348  >loss\n",
      "2.302003223901521  >loss\n",
      "2.2983631377047176  >loss\n",
      "2.2977922961707398  >loss\n",
      "2.3017470873358383  >loss\n",
      "2.3018396707652045  >loss\n",
      "2.29907107057716  >loss\n",
      "2.3009818975776346  >loss\n",
      "2.2966759952787745  >loss\n",
      "2.3017213732731094  >loss\n",
      "2.2967585664129695  >loss\n",
      "2.299994126140155  >loss\n",
      "2.3006271415214696  >loss\n",
      "2.299926486340118  >loss\n",
      "2.305847099776395  >loss\n",
      "2.3015710270617906  >loss\n",
      "2.2982021786529923  >loss\n",
      "2.304166168948027  >loss\n",
      "2.303170052267022  >loss\n",
      "2.2993120767028588  >loss\n",
      "2.3017650945612336  >loss\n",
      "2.299389228380691  >loss\n",
      "2.297947364265873  >loss\n",
      "2.2997841746668994  >loss\n",
      "2.2994089518981533  >loss\n",
      "2.3047396256498573  >loss\n",
      "2.3022002425507284  >loss\n",
      "2.298223668624236  >loss\n",
      "2.298970265294467  >loss\n",
      "2.302630651675238  >loss\n",
      "2.3017471441700685  >loss\n",
      "2.302470876743291  >loss\n",
      "Epoch 0, loss: 2.302471\n",
      "2.301599345776118  >loss\n",
      "2.300996403646111  >loss\n",
      "2.298408384990778  >loss\n",
      "2.298622543160835  >loss\n",
      "2.3007866174014855  >loss\n",
      "2.298529984503482  >loss\n",
      "2.2962061484775638  >loss\n",
      "2.303213667711278  >loss\n",
      "2.299298508526167  >loss\n",
      "2.297273660624056  >loss\n",
      "2.297204997506232  >loss\n",
      "2.29803006323833  >loss\n",
      "2.300234783984722  >loss\n",
      "2.299538465896433  >loss\n",
      "2.2974365914294173  >loss\n",
      "2.2994254709369093  >loss\n",
      "2.302721633316094  >loss\n",
      "2.297165187282787  >loss\n",
      "2.300411915401393  >loss\n",
      "2.3000866543371505  >loss\n",
      "2.297526202151753  >loss\n",
      "2.3008377224257197  >loss\n",
      "2.2998508669004574  >loss\n",
      "2.301206670300561  >loss\n",
      "2.3035516799260303  >loss\n",
      "2.3001855438899703  >loss\n",
      "2.297713986835386  >loss\n",
      "2.3023733051588873  >loss\n",
      "2.2966604317293555  >loss\n",
      "2.300826447904547  >loss\n",
      "2.298819141813131  >loss\n",
      "2.3014910593596487  >loss\n",
      "2.30356673488771  >loss\n",
      "2.2998215437399567  >loss\n",
      "2.2979064357543333  >loss\n",
      "2.2971899186880487  >loss\n",
      "2.297291105871589  >loss\n",
      "2.299663304856177  >loss\n",
      "2.300508560417046  >loss\n",
      "2.3030469060737477  >loss\n",
      "2.293343269881986  >loss\n",
      "2.2997045304432207  >loss\n",
      "2.2980677733754105  >loss\n",
      "2.301843075143586  >loss\n",
      "2.2938053284024122  >loss\n",
      "2.2969945152400695  >loss\n",
      "2.296237748272148  >loss\n",
      "2.297567287731476  >loss\n",
      "2.302141167924411  >loss\n",
      "2.297069399519095  >loss\n",
      "2.2953437465664894  >loss\n",
      "2.2993848843036813  >loss\n",
      "2.3004264790049676  >loss\n",
      "2.2918389793494693  >loss\n",
      "2.300792011487357  >loss\n",
      "2.2973879780312947  >loss\n",
      "2.296079283764629  >loss\n",
      "2.2982214312234484  >loss\n",
      "2.300167602130275  >loss\n",
      "2.29614051101333  >loss\n",
      "2.2982437226720323  >loss\n",
      "2.2973190602768985  >loss\n",
      "2.3001169429106647  >loss\n",
      "2.2995471075994125  >loss\n",
      "2.29983391770162  >loss\n",
      "2.2963562274217173  >loss\n",
      "2.2992568966731888  >loss\n",
      "2.299784613557429  >loss\n",
      "2.298898115899426  >loss\n",
      "2.298556540079945  >loss\n",
      "2.297203627845495  >loss\n",
      "2.294919630905522  >loss\n",
      "2.299859627105903  >loss\n",
      "2.300629637064768  >loss\n",
      "2.2945143162378954  >loss\n",
      "2.2969707061402445  >loss\n",
      "2.29339832109838  >loss\n",
      "2.2945951506192963  >loss\n",
      "2.299131499847449  >loss\n",
      "2.3015641787373924  >loss\n",
      "2.3030518708645253  >loss\n",
      "2.294548843599371  >loss\n",
      "2.294717357904768  >loss\n",
      "2.3011768098046663  >loss\n",
      "2.2961945557210695  >loss\n",
      "2.301131688508469  >loss\n",
      "2.2960414621844984  >loss\n",
      "2.292249723967108  >loss\n",
      "2.297965640827495  >loss\n",
      "2.2975323628127686  >loss\n",
      "Epoch 1, loss: 2.297532\n",
      "2.2963285934049082  >loss\n",
      "2.297497754955056  >loss\n",
      "2.301393243310062  >loss\n",
      "2.3023988993706377  >loss\n",
      "2.29718908841519  >loss\n",
      "2.2931658884286863  >loss\n",
      "2.2933502623997555  >loss\n",
      "2.302050621694133  >loss\n",
      "2.3014673010839872  >loss\n",
      "2.297628001949899  >loss\n",
      "2.302129680318598  >loss\n",
      "2.2968081694976705  >loss\n",
      "2.295559922664306  >loss\n",
      "2.2971265377058536  >loss\n",
      "2.301225504410204  >loss\n",
      "2.2957414391523145  >loss\n",
      "2.291124612248258  >loss\n",
      "2.3000828462777934  >loss\n",
      "2.294235831097228  >loss\n",
      "2.2978677411362036  >loss\n",
      "2.2963203576749383  >loss\n",
      "2.29504596853506  >loss\n",
      "2.2971688668020565  >loss\n",
      "2.2929608300787954  >loss\n",
      "2.299030561139007  >loss\n",
      "2.3016912413858375  >loss\n",
      "2.2945718645402886  >loss\n",
      "2.297613342527138  >loss\n",
      "2.3029872817447288  >loss\n",
      "2.290739088464615  >loss\n",
      "2.295824206707107  >loss\n",
      "2.2959870074537267  >loss\n",
      "2.2997019736106155  >loss\n",
      "2.296364883774879  >loss\n",
      "2.298302714929149  >loss\n",
      "2.2914163697151086  >loss\n",
      "2.2941964043679786  >loss\n",
      "2.291387340239746  >loss\n",
      "2.293137267376631  >loss\n",
      "2.297797407498104  >loss\n",
      "2.2946831458002914  >loss\n",
      "2.2948091210678245  >loss\n",
      "2.2926443212028573  >loss\n",
      "2.3010294068876966  >loss\n",
      "2.301641690054235  >loss\n",
      "2.297262053842415  >loss\n",
      "2.290675486911989  >loss\n",
      "2.2973270846150187  >loss\n",
      "2.294812348455883  >loss\n",
      "2.2960256474658416  >loss\n",
      "2.2904891871271134  >loss\n",
      "2.293142125779316  >loss\n",
      "2.2951616727326236  >loss\n",
      "2.2997406173920427  >loss\n",
      "2.29561168546516  >loss\n",
      "2.2964283607881386  >loss\n",
      "2.2940243299027037  >loss\n",
      "2.29243156509082  >loss\n",
      "2.2960957030008324  >loss\n",
      "2.2952781410293497  >loss\n",
      "2.297178684953887  >loss\n",
      "2.2963592280995155  >loss\n",
      "2.3004750299981533  >loss\n",
      "2.293923950862812  >loss\n",
      "2.294647073123423  >loss\n",
      "2.292753466776001  >loss\n",
      "2.2937582474808242  >loss\n",
      "2.291270805996513  >loss\n",
      "2.294892444238372  >loss\n",
      "2.2920852735299793  >loss\n",
      "2.293291276951694  >loss\n",
      "2.2948731031905485  >loss\n",
      "2.2934367218852745  >loss\n",
      "2.292947332803878  >loss\n",
      "2.2928752282496934  >loss\n",
      "2.295372679597142  >loss\n",
      "2.298598452836207  >loss\n",
      "2.2965443206835356  >loss\n",
      "2.293127771005184  >loss\n",
      "2.2905712526411075  >loss\n",
      "2.2899926387025378  >loss\n",
      "2.2932119144874403  >loss\n",
      "2.2998427088461533  >loss\n",
      "2.2997281704657664  >loss\n",
      "2.301392927283289  >loss\n",
      "2.2971020334875845  >loss\n",
      "2.2954359564511417  >loss\n",
      "2.294399442665633  >loss\n",
      "2.2990209600947447  >loss\n",
      "2.2890926959574505  >loss\n",
      "Epoch 2, loss: 2.289093\n",
      "2.292619689267158  >loss\n",
      "2.295269302825707  >loss\n",
      "2.29081120485747  >loss\n",
      "2.295895948483036  >loss\n",
      "2.287621200323179  >loss\n",
      "2.287640060751186  >loss\n",
      "2.2954847145544788  >loss\n",
      "2.2855043856219095  >loss\n",
      "2.293115065713818  >loss\n",
      "2.292857653501483  >loss\n",
      "2.2913488527016828  >loss\n",
      "2.294581700432428  >loss\n",
      "2.2984800976842585  >loss\n",
      "2.296966754488092  >loss\n",
      "2.2924941136037194  >loss\n",
      "2.2957335732111694  >loss\n",
      "2.291780203871755  >loss\n",
      "2.2935943279386297  >loss\n",
      "2.298736657444175  >loss\n",
      "2.297472260455296  >loss\n",
      "2.2974090733999972  >loss\n",
      "2.2999699463196954  >loss\n",
      "2.2906345673518462  >loss\n",
      "2.294849006724759  >loss\n",
      "2.291620799615269  >loss\n",
      "2.2930514078522846  >loss\n",
      "2.2867603469047504  >loss\n",
      "2.2971504495749646  >loss\n",
      "2.289939940488172  >loss\n",
      "2.2913098426764416  >loss\n",
      "2.2880136311947403  >loss\n",
      "2.2893436295324614  >loss\n",
      "2.2981432287828114  >loss\n",
      "2.301533237246047  >loss\n",
      "2.2901393731421082  >loss\n",
      "2.3007978973683745  >loss\n",
      "2.2895663355140066  >loss\n",
      "2.295660971462149  >loss\n",
      "2.2977650508365293  >loss\n",
      "2.29362703112205  >loss\n",
      "2.29955876188492  >loss\n",
      "2.2963870263081563  >loss\n",
      "2.2923089653141244  >loss\n",
      "2.2892451712349193  >loss\n",
      "2.2900023706309285  >loss\n",
      "2.290787017678351  >loss\n",
      "2.2930188826977984  >loss\n",
      "2.289169134977755  >loss\n",
      "2.298664049828699  >loss\n",
      "2.28921704351911  >loss\n",
      "2.2858247600993304  >loss\n",
      "2.2912898903522625  >loss\n",
      "2.294849666209707  >loss\n",
      "2.287719932505659  >loss\n",
      "2.2903246654888725  >loss\n",
      "2.294609522321074  >loss\n",
      "2.2938493310042056  >loss\n",
      "2.294282231318343  >loss\n",
      "2.2867657748038446  >loss\n",
      "2.295668448511295  >loss\n",
      "2.290024961412179  >loss\n",
      "2.2994916686994977  >loss\n",
      "2.2979302207212897  >loss\n",
      "2.2953496552048502  >loss\n",
      "2.290018967720133  >loss\n",
      "2.2905104819405335  >loss\n",
      "2.296686546468566  >loss\n",
      "2.2991437138031214  >loss\n",
      "2.291509723599455  >loss\n",
      "2.2931356314226234  >loss\n",
      "2.288652461864302  >loss\n",
      "2.290279931426812  >loss\n",
      "2.2894210353354905  >loss\n",
      "2.300519844273575  >loss\n",
      "2.2888273349654207  >loss\n",
      "2.298950438803784  >loss\n",
      "2.2931362429012703  >loss\n",
      "2.295911421686054  >loss\n",
      "2.290863001503398  >loss\n",
      "2.292657136239299  >loss\n",
      "2.2963602052274057  >loss\n",
      "2.29467980499504  >loss\n",
      "2.2971485570439967  >loss\n",
      "2.2985991153465157  >loss\n",
      "2.289303060736936  >loss\n",
      "2.2947817210244157  >loss\n",
      "2.2902253181023338  >loss\n",
      "2.2949347210275994  >loss\n",
      "2.2936947364200893  >loss\n",
      "2.292210747781952  >loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, loss: 2.292211\n",
      "2.2955169092843226  >loss\n",
      "2.290601584687825  >loss\n",
      "2.2943940227819395  >loss\n",
      "2.295011933423616  >loss\n",
      "2.296336347079897  >loss\n",
      "2.2930157346418594  >loss\n",
      "2.287553409838385  >loss\n",
      "2.2854316174990132  >loss\n",
      "2.291283301560588  >loss\n",
      "2.2913595874627966  >loss\n",
      "2.291244725895336  >loss\n",
      "2.2902911695392536  >loss\n",
      "2.2852588006532124  >loss\n",
      "2.2920645904440846  >loss\n",
      "2.2934879154451377  >loss\n",
      "2.292092182959474  >loss\n",
      "2.284835601101475  >loss\n",
      "2.2890635521978044  >loss\n",
      "2.283549612893292  >loss\n",
      "2.2916286122696103  >loss\n",
      "2.2910843393833953  >loss\n",
      "2.28722246107344  >loss\n",
      "2.296343424134724  >loss\n",
      "2.289670368824178  >loss\n",
      "2.2874755493471217  >loss\n",
      "2.295140544833038  >loss\n",
      "2.288610142878921  >loss\n",
      "2.2908779588728896  >loss\n",
      "2.2929233883026434  >loss\n",
      "2.2909953188321  >loss\n",
      "2.292081922396754  >loss\n",
      "2.287543007647475  >loss\n",
      "2.297820091516324  >loss\n",
      "2.3007195342563334  >loss\n",
      "2.3029046890491838  >loss\n",
      "2.2850760733742024  >loss\n",
      "2.2931348670820624  >loss\n",
      "2.290456088934785  >loss\n",
      "2.2958847483729454  >loss\n",
      "2.292336280663768  >loss\n",
      "2.2843909777859768  >loss\n",
      "2.294620501619381  >loss\n",
      "2.2908842729021752  >loss\n",
      "2.2947015680435405  >loss\n",
      "2.2907340622652597  >loss\n",
      "2.292713906698325  >loss\n",
      "2.2942150525837826  >loss\n",
      "2.2883282739418855  >loss\n",
      "2.2886040761565773  >loss\n",
      "2.2904536946262524  >loss\n",
      "2.292081291823463  >loss\n",
      "2.2939659592538435  >loss\n",
      "2.2878619366311193  >loss\n",
      "2.291636247877884  >loss\n",
      "2.293711336245342  >loss\n",
      "2.2892593031114767  >loss\n",
      "2.300262311519235  >loss\n",
      "2.2974555887935244  >loss\n",
      "2.288504312698517  >loss\n",
      "2.2882657113442137  >loss\n",
      "2.2928299314170064  >loss\n",
      "2.295302322811499  >loss\n",
      "2.2922502640764355  >loss\n",
      "2.2957492200104292  >loss\n",
      "2.2919229373839203  >loss\n",
      "2.293336132149195  >loss\n",
      "2.2839739320358667  >loss\n",
      "2.2901678631980493  >loss\n",
      "2.2951962186803474  >loss\n",
      "2.2890528381913695  >loss\n",
      "2.2876975774983594  >loss\n",
      "2.291080746476817  >loss\n",
      "2.2948072393231294  >loss\n",
      "2.284737537028371  >loss\n",
      "2.2842541324637526  >loss\n",
      "2.2830352423811364  >loss\n",
      "2.290009495457986  >loss\n",
      "2.2874863838091235  >loss\n",
      "2.2878393682354345  >loss\n",
      "2.28519987484638  >loss\n",
      "2.2882136346363477  >loss\n",
      "2.2985273688438803  >loss\n",
      "2.291355668611819  >loss\n",
      "2.2895846144520986  >loss\n",
      "2.286879484302678  >loss\n",
      "2.288727857776463  >loss\n",
      "2.2843942635846783  >loss\n",
      "2.2927639178936756  >loss\n",
      "2.2822857485977988  >loss\n",
      "2.2902780026420877  >loss\n",
      "Epoch 4, loss: 2.290278\n",
      "2.295666830134984  >loss\n",
      "2.2914562099446973  >loss\n",
      "2.2954692931251617  >loss\n",
      "2.2864886485956646  >loss\n",
      "2.2908907735317827  >loss\n",
      "2.2864526041648503  >loss\n",
      "2.286835871003309  >loss\n",
      "2.300239793385998  >loss\n",
      "2.291452294478596  >loss\n",
      "2.2871215562920333  >loss\n",
      "2.2877958052134435  >loss\n",
      "2.284830004427864  >loss\n",
      "2.2950463784372297  >loss\n",
      "2.2893603345708256  >loss\n",
      "2.2867056130739414  >loss\n",
      "2.2797587018221592  >loss\n",
      "2.2872453481684207  >loss\n",
      "2.2925765782837306  >loss\n",
      "2.2919809215940856  >loss\n",
      "2.2961249960873658  >loss\n",
      "2.288180878070876  >loss\n",
      "2.2907285459750346  >loss\n",
      "2.292691436163973  >loss\n",
      "2.2849058121809662  >loss\n",
      "2.2898714125802293  >loss\n",
      "2.2895716053273873  >loss\n",
      "2.294177757517608  >loss\n",
      "2.286365881107748  >loss\n",
      "2.2842150576203673  >loss\n",
      "2.2886045535733475  >loss\n",
      "2.2888064717630194  >loss\n",
      "2.285896215419451  >loss\n",
      "2.295606962271985  >loss\n",
      "2.2845396577201833  >loss\n",
      "2.2863331674319367  >loss\n",
      "2.292146229600878  >loss\n",
      "2.2831251352998785  >loss\n",
      "2.2928057472787113  >loss\n",
      "2.2802140746475463  >loss\n",
      "2.2846417376624397  >loss\n",
      "2.281985526042167  >loss\n",
      "2.2896216033582215  >loss\n",
      "2.284306099184517  >loss\n",
      "2.289924367473056  >loss\n",
      "2.2898691256490933  >loss\n",
      "2.289151591362417  >loss\n",
      "2.284655151768334  >loss\n",
      "2.2876066103299855  >loss\n",
      "2.2874850922235344  >loss\n",
      "2.2886552610577016  >loss\n",
      "2.2878432953791585  >loss\n",
      "2.287725598009988  >loss\n",
      "2.293403252654152  >loss\n",
      "2.2815583085589295  >loss\n",
      "2.2860712039089814  >loss\n",
      "2.2852269391973254  >loss\n",
      "2.2912322122286826  >loss\n",
      "2.288811249999618  >loss\n",
      "2.2880038786167916  >loss\n",
      "2.2870278436795304  >loss\n",
      "2.2950697941594225  >loss\n",
      "2.274263838425894  >loss\n",
      "2.2921926415270817  >loss\n",
      "2.2935894680191455  >loss\n",
      "2.2946667174765345  >loss\n",
      "2.2916484564726116  >loss\n",
      "2.284722214746444  >loss\n",
      "2.2906628367060144  >loss\n",
      "2.2896581234245845  >loss\n",
      "2.284770483807779  >loss\n",
      "2.2904008155272706  >loss\n",
      "2.2813412575390455  >loss\n",
      "2.284779952151848  >loss\n",
      "2.284847763106119  >loss\n",
      "2.2883086985389243  >loss\n",
      "2.2883931074681776  >loss\n",
      "2.285441086911929  >loss\n",
      "2.2894228425440506  >loss\n",
      "2.291627658175122  >loss\n",
      "2.2872503547628273  >loss\n",
      "2.2983830385043578  >loss\n",
      "2.28246406581814  >loss\n",
      "2.294639393609665  >loss\n",
      "2.287510556167791  >loss\n",
      "2.284363750213364  >loss\n",
      "2.284401710632813  >loss\n",
      "2.2783811357501027  >loss\n",
      "2.2882919678959777  >loss\n",
      "2.2912070728355807  >loss\n",
      "2.2927037913310415  >loss\n",
      "Epoch 5, loss: 2.292704\n",
      "2.283512132358457  >loss\n",
      "2.289425586550543  >loss\n",
      "2.276635220371221  >loss\n",
      "2.2825606872249358  >loss\n",
      "2.2887643637014103  >loss\n",
      "2.2868131723896266  >loss\n",
      "2.2845590041272215  >loss\n",
      "2.2998616111306105  >loss\n",
      "2.2884799585043694  >loss\n",
      "2.2946322158265313  >loss\n",
      "2.288736135052624  >loss\n",
      "2.2905537743877153  >loss\n",
      "2.2910694511443186  >loss\n",
      "2.288982389997907  >loss\n",
      "2.288841316489085  >loss\n",
      "2.2873595770584  >loss\n",
      "2.2851212922340403  >loss\n",
      "2.277837791772591  >loss\n",
      "2.2862915748532604  >loss\n",
      "2.2883472942937346  >loss\n",
      "2.2832534534445363  >loss\n",
      "2.2895602020517294  >loss\n",
      "2.276088309487189  >loss\n",
      "2.2842085194262776  >loss\n",
      "2.2899718734791326  >loss\n",
      "2.280025287732221  >loss\n",
      "2.2805558453660533  >loss\n",
      "2.280894821121512  >loss\n",
      "2.2867512718926406  >loss\n",
      "2.2888931621641317  >loss\n",
      "2.282717199128221  >loss\n",
      "2.280629492965085  >loss\n",
      "2.3030135358909605  >loss\n",
      "2.29579729686011  >loss\n",
      "2.2925055090799735  >loss\n",
      "2.2917919821693977  >loss\n",
      "2.285532794577716  >loss\n",
      "2.2959355669055146  >loss\n",
      "2.286124752415202  >loss\n",
      "2.28750282436823  >loss\n",
      "2.290127804780036  >loss\n",
      "2.2815610600079337  >loss\n",
      "2.299390190309371  >loss\n",
      "2.2824714268651056  >loss\n",
      "2.28809438179766  >loss\n",
      "2.288551095184913  >loss\n",
      "2.2939869275177913  >loss\n",
      "2.2953367001058944  >loss\n",
      "2.2835901085353427  >loss\n",
      "2.276804466791094  >loss\n",
      "2.2966994248577604  >loss\n",
      "2.2856026623642234  >loss\n",
      "2.296118529272885  >loss\n",
      "2.2820036190767063  >loss\n",
      "2.2751750172573297  >loss\n",
      "2.278512201491727  >loss\n",
      "2.2927239582650674  >loss\n",
      "2.2888581448059426  >loss\n",
      "2.2967639999132543  >loss\n",
      "2.288766553096278  >loss\n",
      "2.2875830796238317  >loss\n",
      "2.2734484610214634  >loss\n",
      "2.287412563952447  >loss\n",
      "2.281423914864155  >loss\n",
      "2.2790921539024174  >loss\n",
      "2.2820068934854034  >loss\n",
      "2.2836072020446956  >loss\n",
      "2.278036745991717  >loss\n",
      "2.291324677052577  >loss\n",
      "2.2841529087990273  >loss\n",
      "2.282951914843986  >loss\n",
      "2.2841463831210223  >loss\n",
      "2.2827921597430785  >loss\n",
      "2.2778343266597356  >loss\n",
      "2.2837004378440118  >loss\n",
      "2.282705066761624  >loss\n",
      "2.28536467586712  >loss\n",
      "2.2848823274935315  >loss\n",
      "2.286350140284645  >loss\n",
      "2.2864941044557408  >loss\n",
      "2.286531990518497  >loss\n",
      "2.2907257314976825  >loss\n",
      "2.278880251629743  >loss\n",
      "2.2764939948264287  >loss\n",
      "2.280618516385832  >loss\n",
      "2.2893535803329494  >loss\n",
      "2.281449969917115  >loss\n",
      "2.280394169214717  >loss\n",
      "2.293795411725839  >loss\n",
      "2.2781720212035643  >loss\n",
      "Epoch 6, loss: 2.278172\n",
      "2.285754400506821  >loss\n",
      "2.271801615355716  >loss\n",
      "2.2800489696815083  >loss\n",
      "2.2941844586170332  >loss\n",
      "2.285797251841882  >loss\n",
      "2.2881876406161568  >loss\n",
      "2.2821249943593793  >loss\n",
      "2.278155106164617  >loss\n",
      "2.286303432054133  >loss\n",
      "2.2814784479544237  >loss\n",
      "2.2944715252007004  >loss\n",
      "2.28447354213085  >loss\n",
      "2.28154925657134  >loss\n",
      "2.2852535570997556  >loss\n",
      "2.2790201896313165  >loss\n",
      "2.2870160868636433  >loss\n",
      "2.28086915240352  >loss\n",
      "2.2836364236708824  >loss\n",
      "2.280134325515884  >loss\n",
      "2.2823839008830893  >loss\n",
      "2.2805047181400195  >loss\n",
      "2.29050018312505  >loss\n",
      "2.2922115525283546  >loss\n",
      "2.2830125806666284  >loss\n",
      "2.2788915877970446  >loss\n",
      "2.268009205005146  >loss\n",
      "2.288735902348452  >loss\n",
      "2.2810253686319997  >loss\n",
      "2.2886825124670267  >loss\n",
      "2.271941945566415  >loss\n",
      "2.2864299744452823  >loss\n",
      "2.2821097490700875  >loss\n",
      "2.2742932643559324  >loss\n",
      "2.279025817244986  >loss\n",
      "2.2866215853739575  >loss\n",
      "2.2879937221551643  >loss\n",
      "2.2793518520813834  >loss\n",
      "2.2898908287114113  >loss\n",
      "2.267963361419717  >loss\n",
      "2.285354492577775  >loss\n",
      "2.279392129760883  >loss\n",
      "2.2899751683240392  >loss\n",
      "2.2803846679726414  >loss\n",
      "2.289713094985987  >loss\n",
      "2.28910345668081  >loss\n",
      "2.2879547428199265  >loss\n",
      "2.2757523853597976  >loss\n",
      "2.279259604866337  >loss\n",
      "2.2820293642182947  >loss\n",
      "2.2867077760638828  >loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2887083136458424  >loss\n",
      "2.270125310348924  >loss\n",
      "2.279845471244482  >loss\n",
      "2.2808268156429112  >loss\n",
      "2.2853220379827137  >loss\n",
      "2.284479037801545  >loss\n",
      "2.275743503400829  >loss\n",
      "2.2788016915018736  >loss\n",
      "2.2866182067079395  >loss\n",
      "2.2823041707396756  >loss\n",
      "2.2845268567022123  >loss\n",
      "2.287830888103943  >loss\n",
      "2.2853862348741005  >loss\n",
      "2.2822055043820995  >loss\n",
      "2.2815375613003273  >loss\n",
      "2.2953275030040423  >loss\n",
      "2.2902032437622757  >loss\n",
      "2.2775215265435746  >loss\n",
      "2.295369430376691  >loss\n",
      "2.283405324071748  >loss\n",
      "2.2901400346889367  >loss\n",
      "2.2847991169184256  >loss\n",
      "2.2771050676610964  >loss\n",
      "2.2867996822757872  >loss\n",
      "2.279063399643773  >loss\n",
      "2.2795030808301444  >loss\n",
      "2.2895504582456336  >loss\n",
      "2.2848962672445756  >loss\n",
      "2.2791665244257726  >loss\n",
      "2.285600151980108  >loss\n",
      "2.2891926180839586  >loss\n",
      "2.291266799569097  >loss\n",
      "2.2910736227305843  >loss\n",
      "2.277843707763754  >loss\n",
      "2.289225319504253  >loss\n",
      "2.296569485192795  >loss\n",
      "2.2861962356025836  >loss\n",
      "2.28351324348689  >loss\n",
      "2.2876642253853596  >loss\n",
      "2.2839305681040685  >loss\n",
      "Epoch 7, loss: 2.283931\n",
      "2.285299596271951  >loss\n",
      "2.292130057249604  >loss\n",
      "2.2834131713960546  >loss\n",
      "2.2812876311913053  >loss\n",
      "2.287179797920824  >loss\n",
      "2.2768264240095797  >loss\n",
      "2.2838019472575852  >loss\n",
      "2.279509323367236  >loss\n",
      "2.2810545909620776  >loss\n",
      "2.281469643269648  >loss\n",
      "2.2851131021943405  >loss\n",
      "2.276756137280491  >loss\n",
      "2.2817288345587214  >loss\n",
      "2.2799976050981385  >loss\n",
      "2.2775943595521912  >loss\n",
      "2.288330366315764  >loss\n",
      "2.280938207020592  >loss\n",
      "2.280156527260076  >loss\n",
      "2.2848247812415594  >loss\n",
      "2.287735600695469  >loss\n",
      "2.2819111398226983  >loss\n",
      "2.2927241397673392  >loss\n",
      "2.27917956548303  >loss\n",
      "2.284660668880799  >loss\n",
      "2.28419644544341  >loss\n",
      "2.278475827530482  >loss\n",
      "2.2788603706984762  >loss\n",
      "2.2790071256209057  >loss\n",
      "2.274931167673843  >loss\n",
      "2.2770531535688914  >loss\n",
      "2.2870160274610076  >loss\n",
      "2.2712692022076384  >loss\n",
      "2.273354489632635  >loss\n",
      "2.289390567100523  >loss\n",
      "2.2857147352536837  >loss\n",
      "2.273949859561557  >loss\n",
      "2.2903314082854394  >loss\n",
      "2.2876705603821135  >loss\n",
      "2.273114263880272  >loss\n",
      "2.2722109364781304  >loss\n",
      "2.2749143266124876  >loss\n",
      "2.2813248053054322  >loss\n",
      "2.280648944237136  >loss\n",
      "2.285296678869395  >loss\n",
      "2.2810728724080938  >loss\n",
      "2.2825632842059234  >loss\n",
      "2.27922909448831  >loss\n",
      "2.2853971035841263  >loss\n",
      "2.2776503703065734  >loss\n",
      "2.284393522283303  >loss\n",
      "2.2712179748306913  >loss\n",
      "2.290522640785806  >loss\n",
      "2.282720777415547  >loss\n",
      "2.2866668378563815  >loss\n",
      "2.2899745149973443  >loss\n",
      "2.2916639114661956  >loss\n",
      "2.275589900284399  >loss\n",
      "2.282404689540041  >loss\n",
      "2.2734510005818653  >loss\n",
      "2.2869220619144945  >loss\n",
      "2.277749258560209  >loss\n",
      "2.276141093567605  >loss\n",
      "2.282390073098638  >loss\n",
      "2.275930611378005  >loss\n",
      "2.279056711796619  >loss\n",
      "2.284604321148282  >loss\n",
      "2.2818014312715125  >loss\n",
      "2.2794678866459543  >loss\n",
      "2.2843560257975546  >loss\n",
      "2.2905415650568233  >loss\n",
      "2.2723529427366116  >loss\n",
      "2.2689130696396997  >loss\n",
      "2.2831350151246816  >loss\n",
      "2.276451308211904  >loss\n",
      "2.273640592169509  >loss\n",
      "2.285772516164158  >loss\n",
      "2.286133957514569  >loss\n",
      "2.2744825677588256  >loss\n",
      "2.273193316654047  >loss\n",
      "2.296309803811579  >loss\n",
      "2.287967329984488  >loss\n",
      "2.285336151783846  >loss\n",
      "2.2758435419485417  >loss\n",
      "2.284737954887152  >loss\n",
      "2.287171663989559  >loss\n",
      "2.276341283319764  >loss\n",
      "2.285039643368082  >loss\n",
      "2.2796373832175174  >loss\n",
      "2.2791708326724125  >loss\n",
      "2.2803066196768964  >loss\n",
      "Epoch 8, loss: 2.280307\n",
      "2.277876042674848  >loss\n",
      "2.2788349676406923  >loss\n",
      "2.280250463958329  >loss\n",
      "2.276169314235791  >loss\n",
      "2.2819559168790517  >loss\n",
      "2.284218158548121  >loss\n",
      "2.278755220357018  >loss\n",
      "2.2800993075540026  >loss\n",
      "2.2893736134370757  >loss\n",
      "2.2819713868769016  >loss\n",
      "2.2712585251269637  >loss\n",
      "2.285341334251439  >loss\n",
      "2.2810299621260266  >loss\n",
      "2.2705950055377038  >loss\n",
      "2.27282171271834  >loss\n",
      "2.291647116687882  >loss\n",
      "2.301969238942232  >loss\n",
      "2.274878944184343  >loss\n",
      "2.281033451040528  >loss\n",
      "2.2746463262678986  >loss\n",
      "2.282136734246331  >loss\n",
      "2.284078806304018  >loss\n",
      "2.2751584413812624  >loss\n",
      "2.280532965228977  >loss\n",
      "2.2772882800348597  >loss\n",
      "2.281562590983543  >loss\n",
      "2.276459512641341  >loss\n",
      "2.2717054730986592  >loss\n",
      "2.2851421994037575  >loss\n",
      "2.2852556757475773  >loss\n",
      "2.2834638201321926  >loss\n",
      "2.2813250428019245  >loss\n",
      "2.2787380716759467  >loss\n",
      "2.276949173555111  >loss\n",
      "2.2675319502890874  >loss\n",
      "2.2781414026088957  >loss\n",
      "2.28342586942184  >loss\n",
      "2.274517147027586  >loss\n",
      "2.2808402858019714  >loss\n",
      "2.2812053541909374  >loss\n",
      "2.275102633827604  >loss\n",
      "2.2831908022582774  >loss\n",
      "2.2784468074643276  >loss\n",
      "2.279066084911273  >loss\n",
      "2.2805904137667627  >loss\n",
      "2.2912555805274946  >loss\n",
      "2.2808054496858428  >loss\n",
      "2.280039720991352  >loss\n",
      "2.281612315527854  >loss\n",
      "2.2888658158340225  >loss\n",
      "2.2816666985717626  >loss\n",
      "2.274124986151963  >loss\n",
      "2.270676288756803  >loss\n",
      "2.277747748568876  >loss\n",
      "2.2828191369546573  >loss\n",
      "2.268489342304444  >loss\n",
      "2.2836879623672437  >loss\n",
      "2.2862031624805033  >loss\n",
      "2.2743611247818443  >loss\n",
      "2.282225826972466  >loss\n",
      "2.2792403843845683  >loss\n",
      "2.267312320390263  >loss\n",
      "2.2726008401071316  >loss\n",
      "2.2719582497866107  >loss\n",
      "2.268027453565025  >loss\n",
      "2.2803848445633785  >loss\n",
      "2.2766406004472084  >loss\n",
      "2.2812910563142426  >loss\n",
      "2.290354239723615  >loss\n",
      "2.283367404812  >loss\n",
      "2.2706576010244683  >loss\n",
      "2.2687820267800958  >loss\n",
      "2.2917532220521406  >loss\n",
      "2.257078545649885  >loss\n",
      "2.285253939468921  >loss\n",
      "2.276837413985649  >loss\n",
      "2.2757486291034104  >loss\n",
      "2.286286621331734  >loss\n",
      "2.2799940280522244  >loss\n",
      "2.288342927540733  >loss\n",
      "2.28219222024174  >loss\n",
      "2.2736255655721327  >loss\n",
      "2.2740413729536924  >loss\n",
      "2.2663065295025477  >loss\n",
      "2.275212133349603  >loss\n",
      "2.275534957646671  >loss\n",
      "2.28564165806696  >loss\n",
      "2.2798241590656763  >loss\n",
      "2.2853463100330242  >loss\n",
      "2.289597692742609  >loss\n",
      "Epoch 9, loss: 2.289598\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-5, batch_size=100, reg=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4598887220>]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax((X.dot(W)), axis=1)\n",
    "# X.dot(W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.335705740700437  >loss\n",
      "2882.566265160673  >loss\n",
      "inf  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexprog/Git_repositories/dlcourse_ai/assignments/assignment1/linear_classifer.py:262: RuntimeWarning: divide by zero encountered in log\n",
      "  h = -1 * (np.log(probs))#*probs\n",
      "/home/alexprog/Git_repositories/dlcourse_ai/assignments/assignment1/linear_classifer.py:284: RuntimeWarning: divide by zero encountered in reciprocal\n",
      "  dW = (X_b.T.dot(dp_1)) #/ X_b.shape[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 0, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 1, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 2, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 3, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 4, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 5, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 6, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 7, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 8, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 9, loss: nan\n",
      "Accuracy:  0.049\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 0, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 1, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 2, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 3, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 4, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 5, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 6, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 7, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 8, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 9, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 10, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 11, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 12, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 13, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 14, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 15, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 16, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 17, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 18, loss: nan\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "nan  >loss\n",
      "Epoch 19, loss: nan\n",
      "Accuracy after training for 100 epochs:  0.049\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "l_hist = classifier.fit(train_X, train_y, epochs=20, learning_rate=1e-3, batch_size=300, reg=1e-6)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f459853e430>]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 6]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2,3,6]\n",
    "sorted(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3036583528584917  >loss\n",
      "2.3992682941959065  >loss\n",
      "2.6040089688648984  >loss\n",
      "2.73551316405936  >loss\n",
      "2.6920798736265046  >loss\n",
      "2.8075277714333953  >loss\n",
      "3.072741131300625  >loss\n",
      "3.4691098397467233  >loss\n",
      "2.459330354846211  >loss\n",
      "2.471000796126089  >loss\n",
      "2.5667567250956242  >loss\n",
      "2.7735134693720602  >loss\n",
      "2.790497938359788  >loss\n",
      "2.725383466323838  >loss\n",
      "3.098290149432901  >loss\n",
      "2.9542372305142313  >loss\n",
      "3.119643238527546  >loss\n",
      "3.0710074573492885  >loss\n",
      "3.045518637416403  >loss\n",
      "3.2850906378043323  >loss\n",
      "3.11698805036616  >loss\n",
      "3.0332574179948244  >loss\n",
      "2.907258526966402  >loss\n",
      "2.946529719282073  >loss\n",
      "3.399621399676874  >loss\n",
      "3.5471326695256318  >loss\n",
      "2.882977690534142  >loss\n",
      "3.0372908173248727  >loss\n",
      "3.3471380598554985  >loss\n",
      "3.2436656762725313  >loss\n",
      "Epoch 0, loss: 3.243666\n",
      "2.712547532887931  >loss\n",
      "2.9804065687441503  >loss\n",
      "3.31478858088973  >loss\n",
      "3.1554922933728586  >loss\n",
      "2.7390472761106612  >loss\n",
      "2.8560003071720574  >loss\n",
      "2.940184287766776  >loss\n",
      "2.860948468676183  >loss\n",
      "2.828526213373099  >loss\n",
      "3.0702702514216607  >loss\n",
      "2.8173601041947283  >loss\n",
      "2.8016804264580077  >loss\n",
      "2.6977205579208308  >loss\n",
      "3.2358302400211763  >loss\n",
      "3.422145020323899  >loss\n",
      "2.91210192484356  >loss\n",
      "2.6758977256308563  >loss\n",
      "2.7030760610427946  >loss\n",
      "2.669499439637287  >loss\n",
      "2.824830604350226  >loss\n",
      "3.3080822247715815  >loss\n",
      "3.006439305287607  >loss\n",
      "2.5667774404349957  >loss\n",
      "2.532339265729052  >loss\n",
      "2.7976663611336616  >loss\n",
      "2.7512472733769284  >loss\n",
      "2.811837566016648  >loss\n",
      "2.847233378865271  >loss\n",
      "2.979124769199032  >loss\n",
      "3.0845962965781544  >loss\n",
      "Epoch 1, loss: 3.084596\n",
      "3.382673006285705  >loss\n",
      "3.0671489546657082  >loss\n",
      "3.282628407155327  >loss\n",
      "3.504880589591764  >loss\n",
      "3.4317232074141057  >loss\n",
      "3.3181988867089047  >loss\n",
      "2.724192878971221  >loss\n",
      "2.6655543444325542  >loss\n",
      "2.7958892678416216  >loss\n",
      "3.0753567196864084  >loss\n",
      "3.227244132461277  >loss\n",
      "3.0286328186370133  >loss\n",
      "3.1055477614351568  >loss\n",
      "3.1856845470549544  >loss\n",
      "3.2695156831265275  >loss\n",
      "2.967954137427856  >loss\n",
      "2.6290562802663815  >loss\n",
      "2.587509368698087  >loss\n",
      "3.005923082680824  >loss\n",
      "3.0211722960605276  >loss\n",
      "3.1258427278439607  >loss\n",
      "2.912450283061034  >loss\n",
      "3.110463123408346  >loss\n",
      "2.991574475666923  >loss\n",
      "2.662603978691432  >loss\n",
      "2.6798254283319993  >loss\n",
      "2.6539456632792406  >loss\n",
      "2.565653932150882  >loss\n",
      "2.778093725712855  >loss\n",
      "3.280018211538384  >loss\n",
      "Epoch 2, loss: 3.280018\n",
      "3.0221643320059868  >loss\n",
      "2.796593686435443  >loss\n",
      "3.125664864350614  >loss\n",
      "2.6071045757186635  >loss\n",
      "2.511260526019915  >loss\n",
      "2.435776982496674  >loss\n",
      "2.390497909657998  >loss\n",
      "2.4575251691691054  >loss\n",
      "2.298343509948423  >loss\n",
      "2.260543197204076  >loss\n",
      "2.279915656726935  >loss\n",
      "2.3691436413776636  >loss\n",
      "2.3156632488260307  >loss\n",
      "2.4060856036124654  >loss\n",
      "2.4615743111560215  >loss\n",
      "2.765057508652213  >loss\n",
      "2.984283007650419  >loss\n",
      "3.5561404519157724  >loss\n",
      "3.3516082427295757  >loss\n",
      "2.867696462412691  >loss\n",
      "2.720271743984879  >loss\n",
      "3.0352437680736353  >loss\n",
      "3.8010308445224537  >loss\n",
      "2.4083356216784044  >loss\n",
      "3.216268103475223  >loss\n",
      "3.24233755455813  >loss\n",
      "3.265561764102883  >loss\n",
      "3.5042163431787947  >loss\n",
      "3.676651307240505  >loss\n",
      "3.6135331848220713  >loss\n",
      "Epoch 3, loss: 3.613533\n",
      "3.213400545630224  >loss\n",
      "2.778688085917682  >loss\n",
      "2.5781902583567295  >loss\n",
      "2.4414463117361613  >loss\n",
      "2.3674596255100626  >loss\n",
      "2.3338044407243346  >loss\n",
      "2.250650701486108  >loss\n",
      "2.5736758345322204  >loss\n",
      "2.9246059277149405  >loss\n",
      "2.7780935975791086  >loss\n",
      "3.3829021069765526  >loss\n",
      "3.773583739527373  >loss\n",
      "3.4398762406220054  >loss\n",
      "3.144597521577955  >loss\n",
      "3.012269967416249  >loss\n",
      "2.908914123278523  >loss\n",
      "2.9541480706766317  >loss\n",
      "2.5673521228960214  >loss\n",
      "2.4190957795666566  >loss\n",
      "2.331360246304513  >loss\n",
      "2.3252142892326098  >loss\n",
      "2.3671263249232397  >loss\n",
      "2.3715973270702135  >loss\n",
      "2.4630705079808513  >loss\n",
      "2.5459797527988277  >loss\n",
      "2.676031655593046  >loss\n",
      "2.8093720397821262  >loss\n",
      "2.7890528159647308  >loss\n",
      "2.9169091214173037  >loss\n",
      "3.0372970566423234  >loss\n",
      "Epoch 4, loss: 3.037297\n",
      "2.924756162124807  >loss\n",
      "2.875250622464458  >loss\n",
      "3.153498235240748  >loss\n",
      "3.2297732842740863  >loss\n",
      "3.0277981368087343  >loss\n",
      "3.3309138373380827  >loss\n",
      "3.1539088891767895  >loss\n",
      "3.320089109839142  >loss\n",
      "2.865801075468788  >loss\n",
      "2.6836635574371557  >loss\n",
      "3.047106983435613  >loss\n",
      "3.3506157542414265  >loss\n",
      "2.9885665343704955  >loss\n",
      "3.5664936402107856  >loss\n",
      "3.4941980512925075  >loss\n",
      "2.9332503609794074  >loss\n",
      "3.413747028113975  >loss\n",
      "2.855730029018242  >loss\n",
      "2.7712475169381787  >loss\n",
      "2.8998627788153635  >loss\n",
      "3.317031874428417  >loss\n",
      "3.4205301777442845  >loss\n",
      "2.4276413419098057  >loss\n",
      "2.5510880537013065  >loss\n",
      "2.5793003713281597  >loss\n",
      "2.687006091435973  >loss\n",
      "2.89228132272486  >loss\n",
      "2.7430952186116766  >loss\n",
      "2.703382759406935  >loss\n",
      "2.8648115598945143  >loss\n",
      "Epoch 5, loss: 2.864812\n",
      "2.7093909799480285  >loss\n",
      "2.8167355475562084  >loss\n",
      "2.7917059616258952  >loss\n",
      "2.9854743425759622  >loss\n",
      "2.2799076665865567  >loss\n",
      "2.550559792942703  >loss\n",
      "2.5235474298040637  >loss\n",
      "2.4076408721934848  >loss\n",
      "2.4034256710382143  >loss\n",
      "2.313775523892476  >loss\n",
      "2.409184846603883  >loss\n",
      "2.5750560859148024  >loss\n",
      "2.859086461874358  >loss\n",
      "2.9773511936667365  >loss\n",
      "2.9323664977192765  >loss\n",
      "2.7886197720784884  >loss\n",
      "2.814714693051422  >loss\n",
      "2.664249229216891  >loss\n",
      "2.826448807567273  >loss\n",
      "2.982370056730743  >loss\n",
      "3.0167906904247963  >loss\n",
      "2.8709987002909156  >loss\n",
      "2.776950842547296  >loss\n",
      "2.54742708839442  >loss\n",
      "2.62302801261931  >loss\n",
      "2.645297801675394  >loss\n",
      "2.6369933855647454  >loss\n",
      "2.3718944631310697  >loss\n",
      "2.333135978471795  >loss\n",
      "2.411394975713666  >loss\n",
      "Epoch 6, loss: 2.411395\n",
      "2.325224722945431  >loss\n",
      "2.429774060617088  >loss\n",
      "2.6456128983666884  >loss\n",
      "2.966028433768713  >loss\n",
      "3.296531153552305  >loss\n",
      "3.2945616100750663  >loss\n",
      "3.081568913677875  >loss\n",
      "2.6530903233169854  >loss\n",
      "3.1475054876828796  >loss\n",
      "2.606271053517369  >loss\n",
      "2.7582217738455514  >loss\n",
      "2.805490048722937  >loss\n",
      "3.517758918194552  >loss\n",
      "2.837738575446444  >loss\n",
      "2.6579603326536234  >loss\n",
      "2.7108923385266  >loss\n",
      "2.6958844402505786  >loss\n",
      "3.1607717071024286  >loss\n",
      "2.657394604542203  >loss\n",
      "2.784107509536861  >loss\n",
      "2.773558461521665  >loss\n",
      "2.8113650167808433  >loss\n",
      "2.9791694322031916  >loss\n",
      "2.5833782597087884  >loss\n",
      "2.4944509070624146  >loss\n",
      "2.5692703835660087  >loss\n",
      "2.7571527545786134  >loss\n",
      "2.75369556705464  >loss\n",
      "2.771431100239663  >loss\n",
      "3.152213223712686  >loss\n",
      "Epoch 7, loss: 3.152213\n",
      "3.5224275091444555  >loss\n",
      "3.5327549785166026  >loss\n",
      "3.42319182393024  >loss\n",
      "3.0103270127176076  >loss\n",
      "2.9969978093668646  >loss\n",
      "2.7860536090896972  >loss\n",
      "2.9135650658156984  >loss\n",
      "3.4280178489641964  >loss\n",
      "3.0952772742681525  >loss\n",
      "2.8219235073531714  >loss\n",
      "3.0775717374693556  >loss\n",
      "3.4036142897528303  >loss\n",
      "2.770281627523939  >loss\n",
      "2.5213359972119913  >loss\n",
      "2.3481294393657195  >loss\n",
      "2.3495485605907462  >loss\n",
      "2.4159408969730434  >loss\n",
      "2.3211121165444255  >loss\n",
      "2.215224697830088  >loss\n",
      "2.3471687164282407  >loss\n",
      "2.3121137733219226  >loss\n",
      "2.3669637636862966  >loss\n",
      "2.336219742225189  >loss\n",
      "2.5846121963175777  >loss\n",
      "2.659982645317011  >loss\n",
      "2.3443287724212665  >loss\n",
      "2.6121859240309298  >loss\n",
      "2.9318816691478107  >loss\n",
      "2.827474038352603  >loss\n",
      "2.684678689193925  >loss\n",
      "Epoch 8, loss: 2.684679\n",
      "2.5681898060506008  >loss\n",
      "2.5512484935148962  >loss\n",
      "2.366069521916625  >loss\n",
      "2.1814338891628253  >loss\n",
      "2.1731479761464954  >loss\n",
      "2.3156148189704604  >loss\n",
      "2.2579275363349134  >loss\n",
      "2.3484492745698513  >loss\n",
      "2.3421671061982496  >loss\n",
      "2.4833562035839427  >loss\n",
      "2.6269427598813726  >loss\n",
      "3.1657833075897672  >loss\n",
      "2.875977122790126  >loss\n",
      "2.765769168307052  >loss\n",
      "2.969479226383826  >loss\n",
      "2.9381320710131154  >loss\n",
      "2.864744098440881  >loss\n",
      "2.771389531562479  >loss\n",
      "2.583710530085296  >loss\n",
      "2.5435247644170538  >loss\n",
      "2.873858547017313  >loss\n",
      "2.884109780709277  >loss\n",
      "3.278060226457524  >loss\n",
      "3.3075215106015334  >loss\n",
      "3.4805158646386407  >loss\n",
      "2.787511122049095  >loss\n",
      "3.035557590896976  >loss\n",
      "3.5233029995000487  >loss\n",
      "2.744871505137537  >loss\n",
      "2.658952041279699  >loss\n",
      "Epoch 9, loss: 2.658952\n",
      "2.688559939476977  >loss\n",
      "2.9957212364070505  >loss\n",
      "3.0050781093129553  >loss\n",
      "3.418105109350735  >loss\n",
      "3.702229680231901  >loss\n",
      "3.554930792262783  >loss\n",
      "3.1905822668925943  >loss\n",
      "3.611617551799019  >loss\n",
      "3.8824401414158434  >loss\n",
      "3.119233591374289  >loss\n",
      "3.26254738596986  >loss\n",
      "3.1310333911802632  >loss\n",
      "2.6730796330844853  >loss\n",
      "2.8350484084780843  >loss\n",
      "2.874253472252967  >loss\n",
      "2.972914765780104  >loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.852717007081704  >loss\n",
      "3.1299031784536493  >loss\n",
      "2.5863188763891625  >loss\n",
      "2.4583514996111697  >loss\n",
      "2.340506209176247  >loss\n",
      "2.335964555377379  >loss\n",
      "2.4868648254509873  >loss\n",
      "2.5281794699685682  >loss\n",
      "2.6561369795723992  >loss\n",
      "3.4163117293914818  >loss\n",
      "2.709133527213679  >loss\n",
      "2.6263852052918475  >loss\n",
      "2.4091526115886643  >loss\n",
      "2.559485952194741  >loss\n",
      "Epoch 10, loss: 2.559486\n",
      "2.4984368219755693  >loss\n",
      "2.784491890254609  >loss\n",
      "2.6840234381368875  >loss\n",
      "3.132269091196941  >loss\n",
      "2.5421914661481644  >loss\n",
      "2.431469969557681  >loss\n",
      "2.3245686385947884  >loss\n",
      "2.399619381936859  >loss\n",
      "2.3649151605968437  >loss\n",
      "2.4610515233394694  >loss\n",
      "2.425151622724993  >loss\n",
      "2.583062261382506  >loss\n",
      "2.4267847242425256  >loss\n",
      "2.6213433667276202  >loss\n",
      "2.762291871048024  >loss\n",
      "2.936847874566993  >loss\n",
      "2.9584313390418804  >loss\n",
      "3.157288348614202  >loss\n",
      "2.9627817374192524  >loss\n",
      "3.107235187658062  >loss\n",
      "3.291425052422352  >loss\n",
      "3.2430028932050927  >loss\n",
      "2.370054540651506  >loss\n",
      "2.301527279269534  >loss\n",
      "2.913195269324546  >loss\n",
      "2.968957919675419  >loss\n",
      "2.617234714026523  >loss\n",
      "2.5684073220524075  >loss\n",
      "2.5000913724105787  >loss\n",
      "2.446247011428596  >loss\n",
      "Epoch 11, loss: 2.446247\n",
      "2.6257884874203268  >loss\n",
      "2.806288089894982  >loss\n",
      "3.0193686639501442  >loss\n",
      "2.891614402154133  >loss\n",
      "2.8402614135326534  >loss\n",
      "2.886131605175393  >loss\n",
      "3.1743084217373463  >loss\n",
      "2.8781646728171455  >loss\n",
      "2.7106063905505975  >loss\n",
      "2.8571011634200083  >loss\n",
      "2.9283320481431976  >loss\n",
      "2.7883861826986314  >loss\n",
      "3.063336102550397  >loss\n",
      "3.2923862284172447  >loss\n",
      "3.209885539036541  >loss\n",
      "2.824515248935808  >loss\n",
      "2.937309604688334  >loss\n",
      "2.8064090343508825  >loss\n",
      "3.0988220781716955  >loss\n",
      "3.3973925204751256  >loss\n",
      "3.228935397443929  >loss\n",
      "2.768677538682291  >loss\n",
      "2.7811422912119412  >loss\n",
      "2.4283775379108294  >loss\n",
      "2.9815416586890815  >loss\n",
      "3.133756991232874  >loss\n",
      "3.0113235813503265  >loss\n",
      "2.4816653685436987  >loss\n",
      "2.7295444066168617  >loss\n",
      "2.770602704326693  >loss\n",
      "Epoch 12, loss: 2.770603\n",
      "2.7020214928959407  >loss\n",
      "2.4729718505952207  >loss\n",
      "2.5350700094045058  >loss\n",
      "2.3873079597515297  >loss\n",
      "2.382106071812412  >loss\n",
      "2.4124071880535243  >loss\n",
      "2.6379227644749776  >loss\n",
      "2.639209613711838  >loss\n",
      "2.93734629066051  >loss\n",
      "2.6103223333264203  >loss\n",
      "2.454166758731476  >loss\n",
      "2.5552112525298183  >loss\n",
      "2.3564861640726487  >loss\n",
      "2.3936374873642996  >loss\n",
      "2.3131499780597067  >loss\n",
      "2.383096623077942  >loss\n",
      "2.485377713909878  >loss\n",
      "2.3945327421385545  >loss\n",
      "2.344081872366491  >loss\n",
      "2.311410468107338  >loss\n",
      "2.3030790561854135  >loss\n",
      "2.3470127909247185  >loss\n",
      "2.3543439510254958  >loss\n",
      "2.355028690821554  >loss\n",
      "2.7813045180865474  >loss\n",
      "2.846191749403646  >loss\n",
      "3.141980725923795  >loss\n",
      "3.250111976514938  >loss\n",
      "2.9953209789413786  >loss\n",
      "2.919278666585135  >loss\n",
      "Epoch 13, loss: 2.919279\n",
      "2.819967939314726  >loss\n",
      "3.180395368275144  >loss\n",
      "2.412614426077707  >loss\n",
      "2.4401214986085735  >loss\n",
      "2.5564946081046354  >loss\n",
      "2.460651088871948  >loss\n",
      "2.760574667610099  >loss\n",
      "2.9552049126558453  >loss\n",
      "2.3334325939389555  >loss\n",
      "2.3511040966837427  >loss\n",
      "2.656098999349277  >loss\n",
      "3.0871027437162395  >loss\n",
      "2.7294644598668367  >loss\n",
      "2.4428156548573865  >loss\n",
      "2.3935209842003404  >loss\n",
      "2.3578326010203683  >loss\n",
      "2.467846753173266  >loss\n",
      "2.502806454465036  >loss\n",
      "2.4015852138046165  >loss\n",
      "2.4155738984229917  >loss\n",
      "2.801130323091649  >loss\n",
      "3.40493976103842  >loss\n",
      "2.699906116706953  >loss\n",
      "2.7259830141781687  >loss\n",
      "2.95400287281078  >loss\n",
      "3.0620605102865337  >loss\n",
      "2.683538370457742  >loss\n",
      "2.480308136086772  >loss\n",
      "2.5165490569040716  >loss\n",
      "2.5670175199095833  >loss\n",
      "Epoch 14, loss: 2.567018\n",
      "2.539274667589184  >loss\n",
      "2.740989236700269  >loss\n",
      "2.920193758762199  >loss\n",
      "2.7566416979256405  >loss\n",
      "2.808803168778256  >loss\n",
      "2.850224286610835  >loss\n",
      "2.8660976514425793  >loss\n",
      "2.639226353943753  >loss\n",
      "2.782474739088063  >loss\n",
      "2.483898091422061  >loss\n",
      "2.527190295964174  >loss\n",
      "2.729214787182456  >loss\n",
      "2.4784858690828426  >loss\n",
      "2.5146539578366203  >loss\n",
      "2.683706547581171  >loss\n",
      "2.7618538811156648  >loss\n",
      "3.146698315023112  >loss\n",
      "2.63993143865376  >loss\n",
      "2.472100074833408  >loss\n",
      "2.6641487370940276  >loss\n",
      "2.7450460428711327  >loss\n",
      "2.4834299318264272  >loss\n",
      "2.763019040259311  >loss\n",
      "2.636714559124133  >loss\n",
      "2.6163917472343585  >loss\n",
      "2.3959724100717206  >loss\n",
      "2.297391070162389  >loss\n",
      "2.452325522048506  >loss\n",
      "2.568611696448726  >loss\n",
      "2.7178602958973097  >loss\n",
      "Epoch 15, loss: 2.717860\n",
      "2.841889411623813  >loss\n",
      "2.428198991085617  >loss\n",
      "2.5591669289974797  >loss\n",
      "2.6677993101722777  >loss\n",
      "2.5077613933535745  >loss\n",
      "2.627595562835634  >loss\n",
      "2.727325828761772  >loss\n",
      "2.7287595060010625  >loss\n",
      "2.5277238592400755  >loss\n",
      "2.6761973337750535  >loss\n",
      "2.696103374498188  >loss\n",
      "2.930174959031395  >loss\n",
      "3.230376459245587  >loss\n",
      "2.852637013372629  >loss\n",
      "2.7441551793319463  >loss\n",
      "2.330648422407822  >loss\n",
      "2.3489591729483017  >loss\n",
      "2.306260710855366  >loss\n",
      "2.2758596548630328  >loss\n",
      "2.3131485170335413  >loss\n",
      "2.402220997179143  >loss\n",
      "2.9584727214893345  >loss\n",
      "3.005347036722345  >loss\n",
      "2.8351768766772465  >loss\n",
      "2.8473606641363856  >loss\n",
      "2.7190620564572052  >loss\n",
      "3.272773867087791  >loss\n",
      "3.3782368708913397  >loss\n",
      "3.1343574391368856  >loss\n",
      "2.5836744784443635  >loss\n",
      "Epoch 16, loss: 2.583674\n",
      "2.674835447333209  >loss\n",
      "3.019395295980646  >loss\n",
      "2.7520374879998797  >loss\n",
      "3.045653404354875  >loss\n",
      "3.182396173853757  >loss\n",
      "2.748482386991193  >loss\n",
      "2.902634211887258  >loss\n",
      "3.181643896460092  >loss\n",
      "2.9171697527632245  >loss\n",
      "2.604813372161627  >loss\n",
      "2.8615652256872917  >loss\n",
      "2.8162317114897224  >loss\n",
      "2.9129438916502246  >loss\n",
      "2.65158656651847  >loss\n",
      "2.9147960330658695  >loss\n",
      "2.5043946181560797  >loss\n",
      "2.635067521231872  >loss\n",
      "2.800750059958674  >loss\n",
      "2.3208695022468073  >loss\n",
      "2.298106510788036  >loss\n",
      "2.4306987150753754  >loss\n",
      "2.7638818855201546  >loss\n",
      "2.7908762111348615  >loss\n",
      "2.699660553482389  >loss\n",
      "2.730298319325439  >loss\n",
      "3.1459793732492827  >loss\n",
      "3.3383228712271924  >loss\n",
      "3.405315547580122  >loss\n",
      "3.0300140670021687  >loss\n",
      "2.768040314529336  >loss\n",
      "Epoch 17, loss: 2.768040\n",
      "2.805989481442726  >loss\n",
      "2.559466946795745  >loss\n",
      "2.4108295602922016  >loss\n",
      "2.3313792680942202  >loss\n",
      "2.3160439662325345  >loss\n",
      "2.372057633604213  >loss\n",
      "2.371541333114911  >loss\n",
      "2.5149145211599855  >loss\n",
      "2.6489909278128825  >loss\n",
      "2.505535331166223  >loss\n",
      "2.8383728605319343  >loss\n",
      "2.9923307452220365  >loss\n",
      "2.8118617757052924  >loss\n",
      "2.824293812996148  >loss\n",
      "2.9541159774465227  >loss\n",
      "2.5645337859679707  >loss\n",
      "2.6285423815236815  >loss\n",
      "2.7481621401805323  >loss\n",
      "2.5439094896434606  >loss\n",
      "2.5052303081985228  >loss\n",
      "2.5156498050929432  >loss\n",
      "2.339884424136389  >loss\n",
      "2.5018662007837538  >loss\n",
      "2.6735064635563917  >loss\n",
      "2.5278203020860683  >loss\n",
      "2.7295818603057  >loss\n",
      "2.6330675364407674  >loss\n",
      "2.861743640789457  >loss\n",
      "2.456938118330774  >loss\n",
      "2.701299687051115  >loss\n",
      "Epoch 18, loss: 2.701300\n",
      "3.0041330079576722  >loss\n",
      "2.768657919259136  >loss\n",
      "2.927430403538972  >loss\n",
      "3.078856761572352  >loss\n",
      "2.6359160712928067  >loss\n",
      "2.8406711930788364  >loss\n",
      "3.032202160828795  >loss\n",
      "2.8110151995581116  >loss\n",
      "2.4051279953244844  >loss\n",
      "2.593073221011616  >loss\n",
      "2.834766712876259  >loss\n",
      "3.386448478419272  >loss\n",
      "3.9822663571464796  >loss\n",
      "3.570906793242604  >loss\n",
      "3.174694016679167  >loss\n",
      "2.8501932494214905  >loss\n",
      "2.569830954994924  >loss\n",
      "2.544913528293142  >loss\n",
      "2.779171424939383  >loss\n",
      "3.33132113456909  >loss\n",
      "2.6865210930546306  >loss\n",
      "2.794556347346925  >loss\n",
      "2.720376658079126  >loss\n",
      "2.6507460325428176  >loss\n",
      "2.472316694043717  >loss\n",
      "2.7778255265659757  >loss\n",
      "3.1253285243092583  >loss\n",
      "2.959756467351408  >loss\n",
      "2.5402231408379743  >loss\n",
      "2.5752733818807068  >loss\n",
      "Epoch 19, loss: 2.575273\n",
      "3.1643886641073777  >loss\n",
      "3.2739374961379832  >loss\n",
      "2.8541038549457136  >loss\n",
      "2.291722516881309  >loss\n",
      "2.684729158443821  >loss\n",
      "2.754326846461213  >loss\n",
      "2.474368733942108  >loss\n",
      "2.4224456720564795  >loss\n",
      "2.5448829199039795  >loss\n",
      "2.1787358051120864  >loss\n",
      "2.430163531801718  >loss\n",
      "2.412973366311862  >loss\n",
      "2.4794853828465437  >loss\n",
      "2.5248718993810746  >loss\n",
      "2.4536280491793856  >loss\n",
      "2.351279431580632  >loss\n",
      "2.4690731592816197  >loss\n",
      "2.9695766413841667  >loss\n",
      "3.1495770434994417  >loss\n",
      "3.087990502306577  >loss\n",
      "2.9539478256075187  >loss\n",
      "2.8763845823399414  >loss\n",
      "2.472085145672818  >loss\n",
      "2.6685343869970986  >loss\n",
      "2.783965255345005  >loss\n",
      "2.860732580398141  >loss\n",
      "2.8688074173221136  >loss\n",
      "3.085841437688217  >loss\n",
      "2.5063534116895942  >loss\n",
      "2.391177961989925  >loss\n",
      "Epoch 20, loss: 2.391178\n",
      "2.316952268322104  >loss\n",
      "2.4675865909767443  >loss\n",
      "2.336727873053583  >loss\n",
      "2.345852784814201  >loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.809119156972324  >loss\n",
      "2.507502987325964  >loss\n",
      "2.358526846025182  >loss\n",
      "2.305354517319505  >loss\n",
      "2.366938337214552  >loss\n",
      "2.353945417560911  >loss\n",
      "2.4948489479037907  >loss\n",
      "3.0527887961634903  >loss\n",
      "3.0355106928557682  >loss\n",
      "3.3881149466894414  >loss\n",
      "2.830379932330723  >loss\n",
      "3.0370803149543506  >loss\n",
      "3.1345533965418504  >loss\n",
      "2.5357655125702627  >loss\n",
      "2.4988940479596904  >loss\n",
      "2.6607261086012377  >loss\n",
      "2.6072961661665275  >loss\n",
      "2.799529419761301  >loss\n",
      "2.941067657595104  >loss\n",
      "2.9272954615066884  >loss\n",
      "2.885059823203609  >loss\n",
      "2.706877596764034  >loss\n",
      "2.4420151144781848  >loss\n",
      "2.597152447234059  >loss\n",
      "2.9067585181153546  >loss\n",
      "2.580000896718952  >loss\n",
      "Epoch 21, loss: 2.580001\n",
      "2.7865213997615164  >loss\n",
      "2.896238637700683  >loss\n",
      "2.572116019704312  >loss\n",
      "2.933167834834222  >loss\n",
      "2.681334630372467  >loss\n",
      "2.496696069279461  >loss\n",
      "2.3109681596262157  >loss\n",
      "2.241935644801159  >loss\n",
      "2.1453836486332745  >loss\n",
      "2.16795437883291  >loss\n",
      "2.2276752333145664  >loss\n",
      "2.32760014510595  >loss\n",
      "2.2863489687962866  >loss\n",
      "2.287733999428641  >loss\n",
      "2.5913438565976445  >loss\n",
      "2.8649983544273816  >loss\n",
      "3.161130781510324  >loss\n",
      "3.3415554037118986  >loss\n",
      "2.7706216876302134  >loss\n",
      "2.8471933452143614  >loss\n",
      "3.0274322838658563  >loss\n",
      "2.8388899807427594  >loss\n",
      "2.588813026761465  >loss\n",
      "2.6458648365027906  >loss\n",
      "2.5992472262115123  >loss\n",
      "2.5084218500971867  >loss\n",
      "2.439691636352427  >loss\n",
      "2.641802700721472  >loss\n",
      "3.016054001410007  >loss\n",
      "2.716943017293666  >loss\n",
      "Epoch 22, loss: 2.716943\n",
      "2.789122523938282  >loss\n",
      "2.8923334834475023  >loss\n",
      "2.931198827084029  >loss\n",
      "2.7999242423997086  >loss\n",
      "2.7683891611474003  >loss\n",
      "2.680801718289122  >loss\n",
      "2.5594705756168885  >loss\n",
      "2.7483678229254003  >loss\n",
      "3.086369545545456  >loss\n",
      "2.927825936425586  >loss\n",
      "2.6344867635973586  >loss\n",
      "2.822191538672264  >loss\n",
      "3.267344794168004  >loss\n",
      "3.0220445933347424  >loss\n",
      "2.7674658848693423  >loss\n",
      "3.0402920144553818  >loss\n",
      "2.837625117501297  >loss\n",
      "2.8359933967798763  >loss\n",
      "2.6862911432720566  >loss\n",
      "2.7612582103591965  >loss\n",
      "2.6239988828486163  >loss\n",
      "2.6125867816981416  >loss\n",
      "2.8807417158639557  >loss\n",
      "3.1334463357861178  >loss\n",
      "2.8461337268929534  >loss\n",
      "2.9421372378551687  >loss\n",
      "2.6964858064953456  >loss\n",
      "2.822274147328155  >loss\n",
      "2.687537878469448  >loss\n",
      "2.760382464900308  >loss\n",
      "Epoch 23, loss: 2.760382\n",
      "2.542307331321602  >loss\n",
      "2.4337492380197254  >loss\n",
      "2.5258279187046684  >loss\n",
      "2.3837203818371324  >loss\n",
      "2.3799471703205968  >loss\n",
      "2.2479959980717017  >loss\n",
      "2.2055892919803686  >loss\n",
      "2.377289486121992  >loss\n",
      "2.530617737620381  >loss\n",
      "2.4659498011222802  >loss\n",
      "2.4913438804013532  >loss\n",
      "2.7347052606567757  >loss\n",
      "2.756062410632089  >loss\n",
      "2.757285769810465  >loss\n",
      "2.627812749781636  >loss\n",
      "2.761993213015792  >loss\n",
      "3.0231037728801784  >loss\n",
      "3.1420460631205733  >loss\n",
      "2.8277177974530567  >loss\n",
      "2.5497572953345347  >loss\n",
      "2.6509564156112138  >loss\n",
      "3.242353739825056  >loss\n",
      "2.9648260704908207  >loss\n",
      "2.6500521088018894  >loss\n",
      "2.9283701542150578  >loss\n",
      "2.7511650200537514  >loss\n",
      "2.601790365210096  >loss\n",
      "2.7125333642486105  >loss\n",
      "2.812677618593175  >loss\n",
      "2.7553882027505336  >loss\n",
      "Epoch 24, loss: 2.755388\n",
      "2.595277649962436  >loss\n",
      "2.76632451972713  >loss\n",
      "2.572716281609845  >loss\n",
      "2.4885359692010236  >loss\n",
      "2.621531466755156  >loss\n",
      "2.681144884696532  >loss\n",
      "2.998666950549267  >loss\n",
      "3.223446311312743  >loss\n",
      "3.4952583309795737  >loss\n",
      "2.6713077646783865  >loss\n",
      "2.7906325050516787  >loss\n",
      "2.8574809596322117  >loss\n",
      "3.0387662665877473  >loss\n",
      "2.925312663846046  >loss\n",
      "3.148589113713858  >loss\n",
      "2.9740224320521587  >loss\n",
      "2.670473336430174  >loss\n",
      "2.5512907022910976  >loss\n",
      "2.297764986653157  >loss\n",
      "2.269802064157655  >loss\n",
      "2.150131133639224  >loss\n",
      "2.19721067295508  >loss\n",
      "2.1671185785846023  >loss\n",
      "2.228636858408287  >loss\n",
      "2.345957408044485  >loss\n",
      "2.372624348148496  >loss\n",
      "2.571324401861742  >loss\n",
      "2.524397516889359  >loss\n",
      "2.6002703701918595  >loss\n",
      "2.389912368391914  >loss\n",
      "Epoch 25, loss: 2.389912\n",
      "2.296467291741566  >loss\n",
      "2.5906590312460134  >loss\n",
      "2.9200701575201817  >loss\n",
      "2.9193342036566006  >loss\n",
      "2.916334623796276  >loss\n",
      "3.034031275504535  >loss\n",
      "2.943202320923097  >loss\n",
      "2.786388609400768  >loss\n",
      "2.601898318286618  >loss\n",
      "2.768239567373495  >loss\n",
      "2.738616415047408  >loss\n",
      "2.742796549307876  >loss\n",
      "2.6464525270293455  >loss\n",
      "2.427493180375709  >loss\n",
      "2.395068640201781  >loss\n",
      "2.357078474673793  >loss\n",
      "2.3348357361746332  >loss\n",
      "2.3831614760373294  >loss\n",
      "2.4325048175778363  >loss\n",
      "2.4066355046925265  >loss\n",
      "2.7636717191516  >loss\n",
      "2.7911520510168146  >loss\n",
      "2.8946570984874374  >loss\n",
      "3.2676620049093072  >loss\n",
      "2.9741850423949794  >loss\n",
      "2.735025374086566  >loss\n",
      "2.6449078245172872  >loss\n",
      "2.7287929331412375  >loss\n",
      "2.6142352479224567  >loss\n",
      "2.680418644055542  >loss\n",
      "Epoch 26, loss: 2.680419\n",
      "3.0043155719831214  >loss\n",
      "3.2255169262013377  >loss\n",
      "2.812437731372268  >loss\n",
      "2.907151751776558  >loss\n",
      "2.5090278849125265  >loss\n",
      "2.562865321957522  >loss\n",
      "2.5962326365569623  >loss\n",
      "2.556295893324333  >loss\n",
      "2.4888768603915388  >loss\n",
      "2.8351823410894537  >loss\n",
      "2.8353188422010196  >loss\n",
      "2.501917378083552  >loss\n",
      "2.3593086753670933  >loss\n",
      "2.3785533029227786  >loss\n",
      "2.972712531438324  >loss\n",
      "3.4200225325521267  >loss\n",
      "3.101662224242635  >loss\n",
      "3.1947385445502774  >loss\n",
      "2.8853813829237978  >loss\n",
      "2.4429092313948697  >loss\n",
      "2.390515285607623  >loss\n",
      "2.414774582585119  >loss\n",
      "2.553064549037651  >loss\n",
      "2.773678211886967  >loss\n",
      "2.478526927886469  >loss\n",
      "2.4852756418230806  >loss\n",
      "2.6608798869192447  >loss\n",
      "2.5942734395676417  >loss\n",
      "2.7426489134403873  >loss\n",
      "2.3224189287713295  >loss\n",
      "Epoch 27, loss: 2.322419\n",
      "2.4260604800683194  >loss\n",
      "2.527240800247701  >loss\n",
      "3.189828819922101  >loss\n",
      "2.585792103855888  >loss\n",
      "2.528714741014655  >loss\n",
      "2.826034199302452  >loss\n",
      "3.0926567867775105  >loss\n",
      "2.8999778080943597  >loss\n",
      "2.7574200795345933  >loss\n",
      "2.574582775376967  >loss\n",
      "2.8326016526215407  >loss\n",
      "2.382594505828125  >loss\n",
      "2.3156545844536165  >loss\n",
      "2.211452582411622  >loss\n",
      "2.178817968729814  >loss\n",
      "2.1162230360264136  >loss\n",
      "2.308495694188708  >loss\n",
      "2.2718140869435524  >loss\n",
      "2.382260104841256  >loss\n",
      "2.208033890510462  >loss\n",
      "2.307972823924701  >loss\n",
      "2.5679927568062815  >loss\n",
      "3.1797740795857403  >loss\n",
      "3.102853383212569  >loss\n",
      "3.160539390089756  >loss\n",
      "2.7459201424177944  >loss\n",
      "2.624437532285658  >loss\n",
      "2.5583089118785995  >loss\n",
      "2.2672713110618816  >loss\n",
      "2.7205612608713623  >loss\n",
      "Epoch 28, loss: 2.720561\n",
      "3.0256748177864976  >loss\n",
      "2.7975051891357405  >loss\n",
      "2.7978522399129115  >loss\n",
      "2.8645655806027372  >loss\n",
      "3.1432653514182216  >loss\n",
      "3.362778871820941  >loss\n",
      "2.6421997714930754  >loss\n",
      "3.1699151573291475  >loss\n",
      "2.864673155917649  >loss\n",
      "2.500781005811503  >loss\n",
      "2.588373450401086  >loss\n",
      "2.534294489441265  >loss\n",
      "2.2952125694652272  >loss\n",
      "2.6335428719715934  >loss\n",
      "3.0289419380543756  >loss\n",
      "2.662346839652681  >loss\n",
      "2.5364382564971546  >loss\n",
      "2.6388121778607534  >loss\n",
      "2.5474618656241432  >loss\n",
      "2.2545052737885216  >loss\n",
      "2.3882721297149283  >loss\n",
      "2.3247400001471448  >loss\n",
      "2.28905277375335  >loss\n",
      "2.2516431208005927  >loss\n",
      "2.4954363767927488  >loss\n",
      "2.4964190675610314  >loss\n",
      "2.679457710254512  >loss\n",
      "2.7155284595043634  >loss\n",
      "2.645634061156476  >loss\n",
      "2.740748318383838  >loss\n",
      "Epoch 29, loss: 2.740748\n",
      "2.62844260223319  >loss\n",
      "2.448561684991829  >loss\n",
      "2.648938233440871  >loss\n",
      "2.556044528953844  >loss\n",
      "3.2288743916101277  >loss\n",
      "2.4432461117945827  >loss\n",
      "2.4035322967622323  >loss\n",
      "2.2456648101148766  >loss\n",
      "2.212521027470901  >loss\n",
      "2.1143262109346908  >loss\n",
      "2.1666962467683657  >loss\n",
      "2.2038362814000227  >loss\n",
      "2.1585246071458184  >loss\n",
      "2.4307839694368556  >loss\n",
      "2.5034452158756877  >loss\n",
      "2.8047750029635306  >loss\n",
      "3.1889933125486882  >loss\n",
      "2.8390817394053363  >loss\n",
      "3.159365195452917  >loss\n",
      "2.7520567205352173  >loss\n",
      "3.1530955650906765  >loss\n",
      "3.2277832930234567  >loss\n",
      "2.6386965629088532  >loss\n",
      "2.6979388190573435  >loss\n",
      "2.700046400669845  >loss\n",
      "2.7823555323815823  >loss\n",
      "2.8225500353925983  >loss\n",
      "2.669875076290816  >loss\n",
      "3.1571060498346704  >loss\n",
      "3.0352017772336115  >loss\n",
      "Epoch 30, loss: 3.035202\n",
      "2.515714370543656  >loss\n",
      "2.5404840822112957  >loss\n",
      "2.6952509088462393  >loss\n",
      "2.884453814276519  >loss\n",
      "2.749433491285119  >loss\n",
      "2.7147317765764845  >loss\n",
      "2.8470052911395336  >loss\n",
      "2.8171170116010744  >loss\n",
      "2.50420515136813  >loss\n",
      "2.6375980634196092  >loss\n",
      "2.757262440109763  >loss\n",
      "2.6102073807477075  >loss\n",
      "2.699077253781524  >loss\n",
      "2.7620742823412527  >loss\n",
      "2.850924008016118  >loss\n",
      "2.6387053664949542  >loss\n",
      "2.6060172036148024  >loss\n",
      "2.523406070959425  >loss\n",
      "2.9746517396802483  >loss\n",
      "2.9240505726985035  >loss\n",
      "2.5216144417527477  >loss\n",
      "2.3391177783786583  >loss\n",
      "2.3212008034479372  >loss\n",
      "2.2994811182036154  >loss\n",
      "2.373867153074636  >loss\n",
      "2.1916080227681314  >loss\n",
      "2.3100538756189217  >loss\n",
      "2.4204231906861047  >loss\n",
      "2.5134396168126525  >loss\n",
      "2.5787569475418586  >loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, loss: 2.578757\n",
      "2.750277867736754  >loss\n",
      "3.1445240397979712  >loss\n",
      "2.549516183634966  >loss\n",
      "2.4891224076330496  >loss\n",
      "2.3049763594097468  >loss\n",
      "2.4681039223412617  >loss\n",
      "2.935938884542494  >loss\n",
      "3.074950020541246  >loss\n",
      "3.0988795856406997  >loss\n",
      "2.616068491747541  >loss\n",
      "2.4696705178563683  >loss\n",
      "2.4713535060343945  >loss\n",
      "2.459125963184294  >loss\n",
      "2.864532283435333  >loss\n",
      "3.0560665399569307  >loss\n",
      "2.806333717266725  >loss\n",
      "2.7397913282909996  >loss\n",
      "2.7328437289566962  >loss\n",
      "2.612405646264362  >loss\n",
      "2.762958330089388  >loss\n",
      "2.7121808520261994  >loss\n",
      "3.0815666216088626  >loss\n",
      "2.974277341237088  >loss\n",
      "2.669257729021496  >loss\n",
      "2.4463978648953644  >loss\n",
      "2.810704676909106  >loss\n",
      "2.778414451370478  >loss\n",
      "2.7152905481023106  >loss\n",
      "2.7050757978218027  >loss\n",
      "3.053266941152026  >loss\n",
      "Epoch 32, loss: 3.053267\n",
      "3.2410889290855027  >loss\n",
      "2.8161477097246417  >loss\n",
      "2.8894177656598785  >loss\n",
      "3.0003264796348166  >loss\n",
      "3.0553753837582507  >loss\n",
      "3.2503929746555427  >loss\n",
      "3.365944137832325  >loss\n",
      "2.8701307366178344  >loss\n",
      "2.8224400965702947  >loss\n",
      "3.0463020170388795  >loss\n",
      "2.9865244735386125  >loss\n",
      "3.115914163878831  >loss\n",
      "2.7416054997737973  >loss\n",
      "2.8249065808221547  >loss\n",
      "2.9080580431810774  >loss\n",
      "2.78474743344694  >loss\n",
      "2.553711429541364  >loss\n",
      "2.411431396572091  >loss\n",
      "2.4634200568784075  >loss\n",
      "2.6667078092614878  >loss\n",
      "2.800586726805721  >loss\n",
      "3.3589840391046577  >loss\n",
      "3.212695757482823  >loss\n",
      "2.4594803159686465  >loss\n",
      "2.3398862566186045  >loss\n",
      "2.278079624171648  >loss\n",
      "2.2362666738999555  >loss\n",
      "2.3549570666607895  >loss\n",
      "2.390203857998667  >loss\n",
      "2.395512880324702  >loss\n",
      "Epoch 33, loss: 2.395513\n",
      "2.3686536031613454  >loss\n",
      "2.6166261244128486  >loss\n",
      "2.603546242394487  >loss\n",
      "2.6574921326128664  >loss\n",
      "2.617437708865969  >loss\n",
      "2.706274195532897  >loss\n",
      "2.6173918634600817  >loss\n",
      "2.6584232819673805  >loss\n",
      "2.649040402151639  >loss\n",
      "2.7548468421602497  >loss\n",
      "2.502215624551389  >loss\n",
      "2.4308614107017834  >loss\n",
      "2.487741538764303  >loss\n",
      "2.8505855730016676  >loss\n",
      "2.8060535966412608  >loss\n",
      "2.4950542978619974  >loss\n",
      "2.3064925307420823  >loss\n",
      "2.247082707670985  >loss\n",
      "2.233764374405136  >loss\n",
      "2.25914112296768  >loss\n",
      "2.3793954437453584  >loss\n",
      "2.3249157938407383  >loss\n",
      "2.672843277497488  >loss\n",
      "2.5812428752835115  >loss\n",
      "2.357046834446147  >loss\n",
      "2.289411370502226  >loss\n",
      "2.6675627268470703  >loss\n",
      "2.509612063650765  >loss\n",
      "3.214910557335416  >loss\n",
      "2.510476005088534  >loss\n",
      "Epoch 34, loss: 2.510476\n",
      "2.301501334856614  >loss\n",
      "2.3032172001316393  >loss\n",
      "2.2548404282241323  >loss\n",
      "2.3552029415368376  >loss\n",
      "2.202462142286815  >loss\n",
      "2.368751801824479  >loss\n",
      "2.445436188046492  >loss\n",
      "2.6457306555465263  >loss\n",
      "2.6760858517072275  >loss\n",
      "2.265035977434911  >loss\n",
      "2.411637166330088  >loss\n",
      "2.195538330557156  >loss\n",
      "2.1856960215091155  >loss\n",
      "2.409452410972578  >loss\n",
      "2.3832061106031603  >loss\n",
      "2.5218803889218844  >loss\n",
      "2.7850063806824497  >loss\n",
      "3.0480616743575157  >loss\n",
      "2.5799024937891275  >loss\n",
      "2.5261280156443098  >loss\n",
      "3.212529547055269  >loss\n",
      "2.9659304365291637  >loss\n",
      "3.0676223487617262  >loss\n",
      "3.48791440737872  >loss\n",
      "2.9282880265584463  >loss\n",
      "2.7134450851667724  >loss\n",
      "3.228553135441166  >loss\n",
      "3.4312921764936513  >loss\n",
      "3.3076469887250743  >loss\n",
      "3.153576726157064  >loss\n",
      "Epoch 35, loss: 3.153577\n",
      "2.8176998754494123  >loss\n",
      "3.2827667666964033  >loss\n",
      "2.6458170761338815  >loss\n",
      "2.403166229245802  >loss\n",
      "2.3330016196827197  >loss\n",
      "2.7387810687446636  >loss\n",
      "2.784436394137967  >loss\n",
      "2.809455486622732  >loss\n",
      "2.7904022819917893  >loss\n",
      "2.4129909569095163  >loss\n",
      "2.2735837856262795  >loss\n",
      "2.241800366874374  >loss\n",
      "2.3378545997549565  >loss\n",
      "2.2896019980806224  >loss\n",
      "2.244689108677202  >loss\n",
      "2.334693939338346  >loss\n",
      "2.2844487748337206  >loss\n",
      "2.2903418380700495  >loss\n",
      "2.29990109917589  >loss\n",
      "2.7570123488122023  >loss\n",
      "2.743459891350208  >loss\n",
      "2.7044633050832116  >loss\n",
      "2.7355719028428456  >loss\n",
      "2.7409603024096865  >loss\n",
      "2.3074374894517633  >loss\n",
      "2.4413742193771206  >loss\n",
      "2.6189619055898783  >loss\n",
      "2.6767770336486336  >loss\n",
      "2.519935535949542  >loss\n",
      "2.495922574610568  >loss\n",
      "Epoch 36, loss: 2.495923\n",
      "2.1932142797460052  >loss\n",
      "2.303972470815408  >loss\n",
      "2.2531230671175333  >loss\n",
      "2.4892716268441997  >loss\n",
      "2.8047842741032776  >loss\n",
      "3.0755893708396047  >loss\n",
      "3.0600541683763347  >loss\n",
      "2.5039600956048274  >loss\n",
      "2.4596434658283926  >loss\n",
      "2.6893075213819477  >loss\n",
      "2.6692521123324635  >loss\n",
      "2.647074292629976  >loss\n",
      "2.7521544724708007  >loss\n",
      "3.0430905887818387  >loss\n",
      "2.945599840251881  >loss\n",
      "2.7148501955644733  >loss\n",
      "2.2004193462403383  >loss\n",
      "2.376231996448775  >loss\n",
      "2.342366802991931  >loss\n",
      "2.349759144488955  >loss\n",
      "2.6259148344043055  >loss\n",
      "2.819562536156707  >loss\n",
      "2.6052604393922025  >loss\n",
      "2.7765812107483305  >loss\n",
      "2.924360822919922  >loss\n",
      "3.181770589258901  >loss\n",
      "2.821436102359683  >loss\n",
      "2.6518741758338984  >loss\n",
      "2.6238001634121244  >loss\n",
      "2.904750085909495  >loss\n",
      "Epoch 37, loss: 2.904750\n",
      "3.057804201830949  >loss\n",
      "2.8517705038368155  >loss\n",
      "2.4459802888824784  >loss\n",
      "2.3907263999263084  >loss\n",
      "2.707534258058522  >loss\n",
      "2.5865649550414473  >loss\n",
      "2.3973913742002853  >loss\n",
      "2.4116833327074865  >loss\n",
      "2.378565148069208  >loss\n",
      "2.3713143107201975  >loss\n",
      "2.3523275322260964  >loss\n",
      "2.389120047769079  >loss\n",
      "2.6843207519086603  >loss\n",
      "2.9576421788798544  >loss\n",
      "2.9054270696485185  >loss\n",
      "3.0737789849091253  >loss\n",
      "2.6239474739015773  >loss\n",
      "3.0172007967977534  >loss\n",
      "2.5974904558005028  >loss\n",
      "2.376826070853646  >loss\n",
      "2.3343546182379433  >loss\n",
      "2.317970166422146  >loss\n",
      "2.2662083460968634  >loss\n",
      "2.3948061533182297  >loss\n",
      "2.4038137304685456  >loss\n",
      "2.3332194856071946  >loss\n",
      "2.440760178499824  >loss\n",
      "2.4704699337988347  >loss\n",
      "3.0103880429425645  >loss\n",
      "2.7297913287677753  >loss\n",
      "Epoch 38, loss: 2.729791\n",
      "2.350228749776579  >loss\n",
      "2.2811352469415915  >loss\n",
      "2.2442345090776055  >loss\n",
      "2.5528838472854978  >loss\n",
      "3.2704322249072666  >loss\n",
      "2.9885891733909213  >loss\n",
      "2.489417309828834  >loss\n",
      "2.5282063688701477  >loss\n",
      "2.663221094029655  >loss\n",
      "2.610213984463911  >loss\n",
      "2.8074503090559166  >loss\n",
      "2.658683320081241  >loss\n",
      "2.9160234598117354  >loss\n",
      "3.068830731355571  >loss\n",
      "2.469816036324585  >loss\n",
      "2.320607073582478  >loss\n",
      "2.464905020937284  >loss\n",
      "2.5858917856347863  >loss\n",
      "2.5669995799571437  >loss\n",
      "2.8473759986733485  >loss\n",
      "2.4886798502621614  >loss\n",
      "2.6484296656128383  >loss\n",
      "2.7480659337466924  >loss\n",
      "2.769968209040816  >loss\n",
      "2.7674316466696958  >loss\n",
      "2.829576867770347  >loss\n",
      "2.802704049001416  >loss\n",
      "2.58749138841526  >loss\n",
      "2.286643001441643  >loss\n",
      "2.245055417111951  >loss\n",
      "Epoch 39, loss: 2.245055\n",
      "2.259410347053839  >loss\n",
      "2.4821219532500387  >loss\n",
      "2.7057301682454407  >loss\n",
      "2.795545831855388  >loss\n",
      "3.0407510742750232  >loss\n",
      "3.2150826030390873  >loss\n",
      "3.284107659878923  >loss\n",
      "2.679672592965448  >loss\n",
      "2.8478148519441087  >loss\n",
      "2.844138398180959  >loss\n",
      "2.364092456796165  >loss\n",
      "2.7475612510002656  >loss\n",
      "2.9805022404359365  >loss\n",
      "2.8393997801760555  >loss\n",
      "2.653327965142247  >loss\n",
      "2.6325898951289326  >loss\n",
      "2.9062379444165805  >loss\n",
      "2.6967042334062055  >loss\n",
      "2.438762400772117  >loss\n",
      "2.402682898137231  >loss\n",
      "2.4741914247673713  >loss\n",
      "2.6399484470229884  >loss\n",
      "2.838803567630102  >loss\n",
      "2.6103698733438967  >loss\n",
      "2.5656474364561612  >loss\n",
      "2.371532258418796  >loss\n",
      "2.4862749025925646  >loss\n",
      "2.5383626611371093  >loss\n",
      "2.659018998601563  >loss\n",
      "2.707239163024624  >loss\n",
      "Epoch 40, loss: 2.707239\n",
      "2.672344978033654  >loss\n",
      "2.544070660983906  >loss\n",
      "2.6625904501530577  >loss\n",
      "2.5633563806256205  >loss\n",
      "2.321005012484713  >loss\n",
      "2.2344274314902655  >loss\n",
      "2.2798636738121654  >loss\n",
      "2.278999259618714  >loss\n",
      "2.1974498745937043  >loss\n",
      "2.1470665631512085  >loss\n",
      "2.2942809599676823  >loss\n",
      "2.4946045545515876  >loss\n",
      "2.6320125729298995  >loss\n",
      "3.3973455321449744  >loss\n",
      "2.2957314179830552  >loss\n",
      "2.420109259260422  >loss\n",
      "2.5598052086870653  >loss\n",
      "3.4077329249110337  >loss\n",
      "2.816533329261955  >loss\n",
      "2.8352387089686535  >loss\n",
      "2.577753643587532  >loss\n",
      "2.425343781456083  >loss\n",
      "2.4224900230727617  >loss\n",
      "2.38257008776383  >loss\n",
      "2.5315588657082193  >loss\n",
      "2.5936246514547485  >loss\n",
      "2.8051690459299934  >loss\n",
      "2.700809294883439  >loss\n",
      "2.8116504863378387  >loss\n",
      "2.6165813829765567  >loss\n",
      "Epoch 41, loss: 2.616581\n",
      "2.435468876516818  >loss\n",
      "2.74341186363688  >loss\n",
      "2.9615856361332833  >loss\n",
      "2.6292640786852775  >loss\n",
      "2.8918207905830124  >loss\n",
      "2.8208565789751145  >loss\n",
      "2.480725195428793  >loss\n",
      "2.621511085423002  >loss\n",
      "2.529898433377187  >loss\n",
      "2.6191530622822987  >loss\n",
      "2.680662907568702  >loss\n",
      "2.854924509326486  >loss\n",
      "3.2570610045058395  >loss\n",
      "3.2047472352283695  >loss\n",
      "2.492588412023806  >loss\n",
      "2.5878199397309753  >loss\n",
      "2.4950487155524868  >loss\n",
      "2.377344137840395  >loss\n",
      "2.262682002160625  >loss\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.390990881815143  >loss\n",
      "2.2093549499041147  >loss\n",
      "2.4270444192169367  >loss\n",
      "2.6611058836372523  >loss\n",
      "2.6110882664924957  >loss\n",
      "2.5345665119576775  >loss\n",
      "3.081496691549013  >loss\n",
      "2.689211588619681  >loss\n",
      "2.3184692629686228  >loss\n",
      "2.5150758564997666  >loss\n",
      "2.809762313080816  >loss\n",
      "Epoch 42, loss: 2.809762\n",
      "2.6168353133560145  >loss\n",
      "2.8337144647986587  >loss\n",
      "2.6700720938593987  >loss\n",
      "2.316701148949023  >loss\n",
      "2.460466716400154  >loss\n",
      "2.541466280407192  >loss\n",
      "2.480655245783053  >loss\n",
      "2.436841526311781  >loss\n",
      "2.665870635585998  >loss\n",
      "2.8790381847798363  >loss\n",
      "2.9796286461936923  >loss\n",
      "3.076515814450047  >loss\n",
      "3.194864743451706  >loss\n",
      "2.777508173847524  >loss\n",
      "2.535937615248938  >loss\n",
      "2.619822488704425  >loss\n",
      "2.5378163767513366  >loss\n",
      "2.864643951658924  >loss\n",
      "2.8549463660684884  >loss\n",
      "2.7659529388063167  >loss\n",
      "2.7870152073664305  >loss\n",
      "2.522221989799815  >loss\n",
      "2.487369230880599  >loss\n",
      "2.742675054859717  >loss\n",
      "3.0225507559592315  >loss\n",
      "3.104551181835756  >loss\n",
      "2.762438117138373  >loss\n",
      "2.5407331376252027  >loss\n",
      "2.6051686117785113  >loss\n",
      "2.568437905150036  >loss\n",
      "Epoch 43, loss: 2.568438\n",
      "2.550747701725288  >loss\n",
      "2.6334042847173347  >loss\n",
      "2.702811255270734  >loss\n",
      "2.6795155008333866  >loss\n",
      "2.779955442610845  >loss\n",
      "2.758868500918336  >loss\n",
      "2.69929328118468  >loss\n",
      "2.6008215787576554  >loss\n",
      "2.7791286625907707  >loss\n",
      "3.473563949852891  >loss\n",
      "2.8406818814036563  >loss\n",
      "2.6535196786013833  >loss\n",
      "2.5137720248885773  >loss\n",
      "2.315515240948691  >loss\n",
      "2.209681491250347  >loss\n",
      "2.2891422778162025  >loss\n",
      "2.3059426010155395  >loss\n",
      "2.522341340301367  >loss\n",
      "2.527326515582069  >loss\n",
      "2.3453831731668466  >loss\n",
      "2.133791784655943  >loss\n",
      "2.227714859028041  >loss\n",
      "2.4182439487400464  >loss\n",
      "2.3444275273981976  >loss\n",
      "2.2794016846598364  >loss\n",
      "2.2172102841889036  >loss\n",
      "2.3847076460372296  >loss\n",
      "2.271532278358437  >loss\n",
      "2.4266537167818463  >loss\n",
      "2.621097843548273  >loss\n",
      "Epoch 44, loss: 2.621098\n",
      "2.649306354034259  >loss\n",
      "2.533884174202174  >loss\n",
      "2.676920115849088  >loss\n",
      "2.987960138883737  >loss\n",
      "2.9513319038049537  >loss\n",
      "2.836120399704519  >loss\n",
      "2.5022379454379102  >loss\n",
      "2.3547679723861865  >loss\n",
      "2.459393335882168  >loss\n",
      "2.4026691107624214  >loss\n",
      "2.4349786285970585  >loss\n",
      "2.370565310642075  >loss\n",
      "2.449879295797423  >loss\n",
      "2.721938383932963  >loss\n",
      "2.8211146022081866  >loss\n",
      "2.4341350349379542  >loss\n",
      "2.60699670921994  >loss\n",
      "2.8168753589645394  >loss\n",
      "2.702776093476018  >loss\n",
      "2.610887953094087  >loss\n",
      "2.7113513945903227  >loss\n",
      "2.7978973344188542  >loss\n",
      "2.6070325672731736  >loss\n",
      "3.327264563296164  >loss\n",
      "2.6917380293386115  >loss\n",
      "2.7041235116592612  >loss\n",
      "3.064016468608605  >loss\n",
      "3.3735880220886565  >loss\n",
      "2.884114221161168  >loss\n",
      "2.6371655774084055  >loss\n",
      "Epoch 45, loss: 2.637166\n",
      "2.421533359660177  >loss\n",
      "2.297601092722682  >loss\n",
      "2.2355679243598297  >loss\n",
      "2.3945859777239464  >loss\n",
      "2.5016189434954588  >loss\n",
      "2.6588950483696854  >loss\n",
      "2.356876185579169  >loss\n",
      "2.370378977625465  >loss\n",
      "2.5170095228552336  >loss\n",
      "2.59647385174094  >loss\n",
      "2.8397423450796464  >loss\n",
      "3.0432650304500664  >loss\n",
      "3.6400634334282373  >loss\n",
      "2.3801538878453687  >loss\n",
      "2.2964714650716385  >loss\n",
      "2.446818692097614  >loss\n",
      "2.5589472140494536  >loss\n",
      "2.6843464706172013  >loss\n",
      "2.7411262424426948  >loss\n",
      "2.8767603675224245  >loss\n",
      "2.654500275453593  >loss\n",
      "2.824684353331522  >loss\n",
      "2.678269054325768  >loss\n",
      "2.529934490517017  >loss\n",
      "2.5420709285272474  >loss\n",
      "2.9037421327052084  >loss\n",
      "3.0487636158549343  >loss\n",
      "2.938101969940815  >loss\n",
      "2.844119771238725  >loss\n",
      "2.5036030914180003  >loss\n",
      "Epoch 46, loss: 2.503603\n",
      "2.4669696978638136  >loss\n",
      "2.760853685755742  >loss\n",
      "3.5554798329124275  >loss\n",
      "2.6791976867652325  >loss\n",
      "2.587466158827455  >loss\n",
      "2.8812500798417306  >loss\n",
      "2.481571175292012  >loss\n",
      "2.697958899382235  >loss\n",
      "2.457802074625715  >loss\n",
      "2.4781380538389817  >loss\n",
      "2.4391207803560313  >loss\n",
      "2.3822248564401067  >loss\n",
      "2.550773989066666  >loss\n",
      "2.7610483107936745  >loss\n",
      "2.8152280778247523  >loss\n",
      "2.785016172204586  >loss\n",
      "2.936773283086904  >loss\n",
      "3.1633606507670695  >loss\n",
      "2.7453898016178306  >loss\n",
      "2.447463748640748  >loss\n",
      "2.6815262285979355  >loss\n",
      "2.5294947294485053  >loss\n",
      "2.5875971496171095  >loss\n",
      "2.4099749115803366  >loss\n",
      "2.35215590293085  >loss\n",
      "2.227592038106844  >loss\n",
      "2.628808875696507  >loss\n",
      "2.7635317471672125  >loss\n",
      "2.762502786832534  >loss\n",
      "2.820224315547377  >loss\n",
      "Epoch 47, loss: 2.820224\n",
      "2.382517237694395  >loss\n",
      "2.5191538706026355  >loss\n",
      "2.5138273965515117  >loss\n",
      "2.334455934864549  >loss\n",
      "2.453183030273894  >loss\n",
      "2.538682592207209  >loss\n",
      "2.5716983181750273  >loss\n",
      "2.4681696683979486  >loss\n",
      "2.489346569531665  >loss\n",
      "2.3944220742951132  >loss\n",
      "2.764766542447382  >loss\n",
      "2.7173388959922296  >loss\n",
      "2.3833111323605265  >loss\n",
      "2.5706216543591633  >loss\n",
      "2.511729807673764  >loss\n",
      "2.712580971108513  >loss\n",
      "2.5118375420601606  >loss\n",
      "3.140331841869824  >loss\n",
      "3.174659177514928  >loss\n",
      "2.7166810187451764  >loss\n",
      "2.179020967771929  >loss\n",
      "2.1749262923051798  >loss\n",
      "2.117342642091547  >loss\n",
      "2.02948461171218  >loss\n",
      "2.103889582970928  >loss\n",
      "2.1578914357016097  >loss\n",
      "2.204715589272905  >loss\n",
      "2.2707929923810997  >loss\n",
      "2.70478096338831  >loss\n",
      "3.2850328584231847  >loss\n",
      "Epoch 48, loss: 3.285033\n",
      "2.984690782085999  >loss\n",
      "2.5894719556469465  >loss\n",
      "2.715570294408382  >loss\n",
      "2.9916935390975725  >loss\n",
      "3.036250338937107  >loss\n",
      "3.0580365800236224  >loss\n",
      "3.1106311352341707  >loss\n",
      "2.774946880831569  >loss\n",
      "2.6958881423007814  >loss\n",
      "2.8083159210862636  >loss\n",
      "2.8243412984883545  >loss\n",
      "2.929991244170725  >loss\n",
      "2.8307974711722914  >loss\n",
      "2.7255660697107484  >loss\n",
      "2.306351330225296  >loss\n",
      "2.6109581914806452  >loss\n",
      "2.9591615936026026  >loss\n",
      "3.114979325933266  >loss\n",
      "3.0032845060188347  >loss\n",
      "2.551442641184351  >loss\n",
      "2.3038155750846623  >loss\n",
      "2.29216934130347  >loss\n",
      "2.412820867853699  >loss\n",
      "2.4513710683675094  >loss\n",
      "2.7114651258604043  >loss\n",
      "3.163474789294255  >loss\n",
      "2.7896770240948547  >loss\n",
      "2.7916818902815037  >loss\n",
      "2.7721399945920444  >loss\n",
      "2.652363625154606  >loss\n",
      "Epoch 49, loss: 2.652364\n",
      "learning_rate=0.001, reg=0.0001, Accuracy\t: 0.187\n",
      "best validation accuracy achieved: 0.187000\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3]#, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4]#, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "best_clf = []\n",
    "best_acc = []\n",
    "plots = []\n",
    "i = 0\n",
    "for learning_rate, reg in itertools.product(learning_rates, reg_strengths):\n",
    "    \n",
    "    classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "    \n",
    "    l_hist_i = classifier.fit(\n",
    "        train_X, train_y, epochs=num_epochs, learning_rate=learning_rate, batch_size=batch_size, reg=reg)\n",
    "    pred_i = classifier.predict(val_X)\n",
    "    accuracy_i = multiclass_accuracy(pred_i, val_y)\n",
    "    best_clf.append(classifier)\n",
    "    best_acc.append((accuracy_i, i))\n",
    "    plots.append(l_hist_i)\n",
    "    \n",
    "    i += 1\n",
    "    print('learning_rate={}, reg={}, Accuracy\\t: {}'.format(learning_rate, reg, accuracy_i))\n",
    "    \n",
    "best = sorted(best_acc)[-1]\n",
    "best_val_accuracy = best[0]\n",
    "best_classifier = best_clf[best[1]]\n",
    "\n",
    "b_hist = plots[best[1]]\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved: 0.207000\n"
     ]
    }
   ],
   "source": [
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.179000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "# test_pred = classifier.predict(test_X)\n",
    "\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f45986216d0>]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(b_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 300)\n",
      "690.7709241839443\n",
      "691.0638871762108\n",
      "690.9386846194352\n",
      "690.6635597300074\n",
      "690.8223377198838\n",
      "690.8508275072097\n",
      "690.8821068413731\n",
      "690.5881559466872\n",
      "690.8620735712747\n",
      "690.8910550196562\n",
      "690.7159675644227\n",
      "690.5451357405011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-95-b36214258f04>:29: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690.9428677601114\n",
      "690.9379335229087\n",
      "690.636831982748\n",
      "690.6974160869652\n",
      "690.6469604641562\n",
      "690.7374034296702\n",
      "690.6372204715057\n",
      "690.7984643082164\n",
      "690.7315775468021\n",
      "690.6720068554637\n",
      "690.6355063421239\n",
      "690.653609423868\n",
      "690.8866936882906\n",
      "690.6932082002365\n",
      "691.1023636508205\n",
      "690.9698700549659\n",
      "690.7397618463228\n",
      "690.8123658762202\n",
      "[690.7709241839443, 691.0638871762108, 690.9386846194352, 690.6635597300074, 690.8223377198838, 690.8508275072097, 690.8821068413731, 690.5881559466872, 690.8620735712747, 690.8910550196562, 690.7159675644227, 690.5451357405011, 690.9428677601114, 690.9379335229087, 690.636831982748, 690.6974160869652, 690.6469604641562, 690.7374034296702, 690.6372204715057, 690.7984643082164, 690.7315775468021, 690.6720068554637, 690.6355063421239, 690.653609423868, 690.8866936882906, 690.6932082002365, 691.1023636508205, 690.9698700549659, 690.7397618463228, 690.8123658762202]\n",
      "Epoch 0, loss: 690.784226\n",
      "(30, 300)\n",
      "690.8064878637574\n",
      "690.6986753841239\n",
      "690.7156184127408\n",
      "690.7534265094378\n",
      "690.8944448024624\n",
      "690.7882417372616\n",
      "690.6725971524687\n",
      "690.7021064441045\n",
      "690.9391380539857\n",
      "690.6935022160798\n",
      "690.653426730571\n",
      "690.7814470309781\n",
      "690.7508287643406\n",
      "690.7518084040084\n",
      "690.7078779993556\n",
      "690.8082100305425\n",
      "690.5744630896368\n",
      "690.7489782869493\n",
      "690.4684551012265\n",
      "690.9037417020701\n",
      "691.0266867569914\n",
      "690.7567955562959\n",
      "690.6532616832025\n",
      "690.506568830287\n",
      "690.808030135513\n",
      "690.5838542281151\n",
      "690.8700858152115\n",
      "690.9768282059896\n",
      "690.9235640615209\n",
      "690.7119263982822\n",
      "[690.8064878637574, 690.6986753841239, 690.7156184127408, 690.7534265094378, 690.8944448024624, 690.7882417372616, 690.6725971524687, 690.7021064441045, 690.9391380539857, 690.6935022160798, 690.653426730571, 690.7814470309781, 690.7508287643406, 690.7518084040084, 690.7078779993556, 690.8082100305425, 690.5744630896368, 690.7489782869493, 690.4684551012265, 690.9037417020701, 691.0266867569914, 690.7567955562959, 690.6532616832025, 690.506568830287, 690.808030135513, 690.5838542281151, 690.8700858152115, 690.9768282059896, 690.9235640615209, 690.7119263982822]\n",
      "Epoch 1, loss: 690.754369\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-7\n",
    "reg=1e-5\n",
    "epochs=2\n",
    "batch_size=300\n",
    "X = train_X\n",
    "y = train_y\n",
    "\n",
    "num_train = X.shape[0]\n",
    "# num_train\n",
    "num_features = X.shape[1]\n",
    "# num_features\n",
    "num_classes = np.max(y)+1\n",
    "# num_classes\n",
    "# if self.W is None:\n",
    "W = 0.001 * np.random.randn(num_features, num_classes)\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    shuffled_indices = np.arange(num_train)\n",
    "    # shuffled_indices\n",
    "    np.random.shuffle(shuffled_indices)\n",
    "    sections = np.arange(batch_size, num_train, batch_size)\n",
    "    # sections\n",
    "    # num_train, num_features\n",
    "    batches_indices = np.array_split(shuffled_indices, sections)\n",
    "    print(np.array(batches_indices).shape)\n",
    "    # target_index = np.ones(batch_size, dtype=np.int)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "    losses = []\n",
    "\n",
    "    for batch in batches_indices:\n",
    "        X_b = X[batch]\n",
    "#         W_b = W[batch]\n",
    "#         print(X_b.shape)\n",
    "        predictions = np.dot(X_b, W)\n",
    "        predictions1 = predictions.copy()\n",
    "    #     print(predictions)\n",
    "#         try:\n",
    "#         predictions1 -= np.max(predictions1)    \n",
    "        predictions1 -= np.array(np.max(predictions, axis=1)).reshape(predictions.shape[0], 1)\n",
    "#         print(predictions1)\n",
    "#         exponents = np.exp(predictions1)\n",
    "#         probs = exponents / np.sum(exponents)\n",
    "\n",
    "        probs = np.exp(predictions1) / np.sum(np.exp(predictions1), axis=1, keepdims=True)\n",
    "        h = -1 * (np.log(probs))\n",
    "#         print('h :\\n', h)\n",
    "#         loss = (h[:, target_index])\n",
    "        loss = (h[np.arange(target_index.shape[0]), target_index.T])\n",
    "#         print('loss array :', loss)\n",
    "\n",
    "        loss = np.sum(loss) / loss.shape[0]\n",
    "#         print('loss :', loss)\n",
    "\n",
    "        zeros = np.zeros_like(predictions)\n",
    "#         zerosW = np.zeros_like(W)\n",
    "\n",
    "#         zeros = np.zeros_like(W)\n",
    "\n",
    "        zeros[np.arange(target_index.shape[0]), target_index.T] = 1\n",
    "#         zerosW[np.arange(target_index.shape[0]), target_index.T] = 1\n",
    "#         zeros[:, target_index] = 1\n",
    "\n",
    "        dp =  (probs - zeros)\n",
    "#         print('dp :', dp)\n",
    "#         dw =  (W - zerosW)\n",
    "#         dW = W.dot(dp)\n",
    "        dW = (X_b.T.dot(dp)) / X_b.shape[0]\n",
    "#         (self.p - self.y) / self.y.shape[0]\n",
    "\n",
    "        loss += reg * np.sum(W**2)\n",
    "\n",
    "        W = W - learning_rate * dW\n",
    "        print(loss)\n",
    "        losses.append(loss)\n",
    "\n",
    "    print(losses)\n",
    "    loss1 = sum(losses)/len(losses)\n",
    "#         loss_history.append(loss1)\n",
    "\n",
    "        # TODO implement prediction and gradient over W\n",
    "    # Your final implementation shouldn't have any loops\n",
    "#         except:\n",
    "#             raise Exception(\"Not implemented NOW!!!\")\n",
    "\n",
    "#     return loss, dW\n",
    "\n",
    "    \n",
    "    \n",
    "#     loss_history.append(loss)\n",
    "    print(\"Epoch %i, loss: %f\" % (epoch, loss1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 9, 2, 1, 7, 1, 3, 5, 3, 6, 4, 2, 9, 1, 0], dtype=uint8)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 4, 3, 5, 7, 5, 1, 2, 5, 5, 1, 1, 2, 1])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gradient1(f, x, delta=1e-5, tol = 1e-4):\n",
    "    \n",
    "    assert isinstance(x, np.ndarray)\n",
    "    assert x.dtype == np.float\n",
    "    \n",
    "    orig_x = x.copy()\n",
    "    fx, analytic_grad = f(x)\n",
    "    assert np.all(np.isclose(orig_x, x, tol)), \"Functions shouldn't modify input variables\"\n",
    "\n",
    "    assert analytic_grad.shape == x.shape\n",
    "    analytic_grad = analytic_grad.copy()\n",
    "\n",
    "    # We will go through every dimension of x and compute numeric\n",
    "    # derivative for it\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "#         print('it>>>>>>', np.array([i for i in it]))\n",
    "\n",
    "        analytic_grad_at_ix = analytic_grad[ix]\n",
    "        print('ix :', ix)\n",
    "        print('x :', x) \n",
    "        print('f(x) :', f(x))\n",
    "        numeric_grad_at_ix = 0\n",
    "\n",
    "        # TODO compute value of numeric gradient of f to idx       \n",
    "        orig_x[ix] += delta\n",
    "        f1 = f(orig_x)\n",
    "        orig_x[ix] -= 2 * delta\n",
    "        f2 = f(orig_x)\n",
    "        \n",
    "        numeric_grad_at_ix = (f1[0] - f2[0]) / (2 * delta)\n",
    "        \n",
    "        print('numeric_grad_at_ix :', numeric_grad_at_ix)\n",
    "        \n",
    "        if not np.isclose(numeric_grad_at_ix, analytic_grad_at_ix, tol):\n",
    "            print(\"Gradients are different at %s. Analytic: %2.5f, Numeric: %2.5f\" % (ix, analytic_grad_at_ix, numeric_grad_at_ix))\n",
    "            return False\n",
    "\n",
    "        it.iternext()\n",
    "\n",
    "    print(\"Gradient check passed!\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt \n",
    "import random \n",
    "\n",
    "def generateSample(N, variance=100): \n",
    "    \n",
    "    X = np.matrix(range(N)).T + 1 \n",
    "    Y = np.matrix([random.random() * variance + i * 10 + 900 for i in range(len(X))]).T \n",
    "    \n",
    "    return X, Y \n",
    "\n",
    "\n",
    "def fitModel_gradient(x, y): \n",
    "    \n",
    "    N = len(x) \n",
    "    w = np.zeros((x.shape[1], 1)) \n",
    "    eta = 0.0001 \n",
    "    maxIteration = 100000 \n",
    "    \n",
    "    for i in range(maxIteration): \n",
    "        error = x * w - y \n",
    "        gradient = xT * error / N \n",
    "        w = w - eta * gradient \n",
    "        \n",
    "    return w \n",
    "    \n",
    "    def plotModel(x, y, w): \n",
    "        \n",
    "        plt.plot(x[:,1], y, \"x\") \n",
    "        plt.plot(x[:,1], x * w, \"r-\") \n",
    "        plt.show() \n",
    "        \n",
    "        \n",
    "    def test(N, variance, modelFunction): \n",
    "        \n",
    "        X, Y = generateSample(N, variance) \n",
    "        X = np.hstack([np.matrix(np.ones(len(X))).T, X]) \n",
    "        w = modelFunction(X, Y) \n",
    "        plotModel(X, Y, w) \n",
    "        test(50, 600, fitModel_gradient) \n",
    "        test(50, 1000, fitModel_gradient) \n",
    "        test(100, 200, fitModel_gradient) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
